{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckWwG9pdjl7E"
   },
   "source": [
    "\n",
    "This is a fun but challenging problem set. It will test your python skills, as well as your understanding of the material in class and in the readings. Start early and debug often! Some notes:\n",
    "\n",
    "* Part 1 is meant to be easy, so get through it quickly.\n",
    "* Part 2 (especially 2.1) will be difficult, but it is the lynchpin of this problem set to make sure to do it well and understand what you've done. If you find your gradient descent algorithm is taking more than a few minutes to complete, debug more, compare notes with others, and go to the Lab sessions (especially the sections on vectorized computation and computational efficiency).\n",
    "* Depending on how well you've done 2.1, parts 2.3 and 4.3 will be relatively painless or incredibly painful. \n",
    "\n",
    "* Part 4 (especially 4.3) will be computationally intensive. Don't leave this until the last minute, otherwise your code might be running when the deadline arrives.\n",
    "\n",
    "* Do the extra credit problems last. This can help you increase your scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Bscj7wWjl7I"
   },
   "source": [
    "---\n",
    "\n",
    "## Introduction to the assignment\n",
    "\n",
    "As with the last assignment, you will be using the [Boston Housing Prices Data Set](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "t0_LwTzXjl7J"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "72miT9Zhjl7K"
   },
   "outputs": [],
   "source": [
    "# Load you data the Boston Housing data into a dataframe\n",
    "# MEDV.txt containt the median house values and data.txt the other 13 features\n",
    "# in order [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
    "# Your code here\n",
    "\n",
    "boston = pd.read_csv('data.txt', sep=\" \", header=None)\n",
    "boston.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  \n",
       "0  307.0  15.534711  397.462329  5.715647  \n",
       "1  255.0  17.914131  397.012611  9.338417  \n",
       "2  243.0  17.919989  396.628236  4.142473  \n",
       "3  226.0  18.979527  398.564784  3.239272  \n",
       "4  234.0  18.708888  399.487766  6.115159  "
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDV = pd.read_csv('target.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston['MEDV'] = MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  MEDV  \n",
       "0  307.0  15.534711  397.462329  5.715647  24.0  \n",
       "1  255.0  17.914131  397.012611  9.338417  21.6  \n",
       "2  243.0  17.919989  396.628236  4.142473  34.7  \n",
       "3  226.0  18.979527  398.564784  3.239272  33.4  \n",
       "4  234.0  18.708888  399.487766  6.115159  36.2  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cbXmKF9jl7L"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Getting oriented\n",
    "\n",
    "\n",
    "### 1.1 Use existing libraries\n",
    "\n",
    "Soon, you will write your own gradient descent algorithm, which you will then use to minimize the squared error cost function.  First, however, let's use the canned versions that come with Python, to make sure we understand what we're aiming to achieve.\n",
    "\n",
    "Using the same Boston housing prices dataset, use the [Linear Regression class](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from sklearn or the [OLS class](http://wiki.scipy.org/Cookbook/OLS) from SciPy to explore the relationship between median housing price and number of rooms per house. Do the following:\n",
    "\n",
    "(a) Regress the housing price on the number of rooms per house. Draw a scatter plot of housing price (y-axis) against rooms (x-axis), and draw the regression line in blue.  You might want to make the dots semi-transparent if it improves the presentation of the figure. \n",
    "\n",
    "(b) Regress the housing price on the number of rooms per house and the (number of rooms per house) squared.  Show the (curved) regression line in green. \n",
    "\n",
    "(c) Interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.473\n",
      "Model:                            OLS   Adj. R-squared:                  0.472\n",
      "Method:                 Least Squares   F-statistic:                     452.3\n",
      "Date:                Fri, 11 Mar 2022   Prob (F-statistic):           4.12e-72\n",
      "Time:                        15:19:37   Log-Likelihood:                -1678.2\n",
      "No. Observations:                 506   AIC:                             3360.\n",
      "Df Residuals:                     504   BIC:                             3369.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -35.5762      2.748    -12.944      0.000     -40.976     -30.176\n",
      "RM             8.9599      0.421     21.267      0.000       8.132       9.788\n",
      "==============================================================================\n",
      "Omnibus:                      105.253   Durbin-Watson:                   0.705\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              604.717\n",
      "Skew:                           0.763   Prob(JB):                    4.87e-132\n",
      "Kurtosis:                       8.133   Cond. No.                         61.7\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "olsA = smf.ols(formula = \"MEDV ~ RM\", data=boston, missing=\"drop\").fit()\n",
    "print(olsA.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hxCNK0pTjl7L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuchiayi/opt/anaconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='RM', ylabel='MEDV'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(boston['RM'], boston['MEDV'], ci=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>RM2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.218960</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.629288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.869420</td>\n",
       "      <td>6.875396</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.347275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>15.534711</td>\n",
       "      <td>397.462329</td>\n",
       "      <td>5.715647</td>\n",
       "      <td>24.0</td>\n",
       "      <td>47.271066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.141576</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.315612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549711</td>\n",
       "      <td>6.499894</td>\n",
       "      <td>78.9</td>\n",
       "      <td>5.315684</td>\n",
       "      <td>2.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>17.914131</td>\n",
       "      <td>397.012611</td>\n",
       "      <td>9.338417</td>\n",
       "      <td>21.6</td>\n",
       "      <td>42.248623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.380457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.340354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697928</td>\n",
       "      <td>7.263489</td>\n",
       "      <td>61.1</td>\n",
       "      <td>5.356935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>17.919989</td>\n",
       "      <td>396.628236</td>\n",
       "      <td>4.142473</td>\n",
       "      <td>34.7</td>\n",
       "      <td>52.758270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.313563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.562407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>7.209732</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.103983</td>\n",
       "      <td>3.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>18.979527</td>\n",
       "      <td>398.564784</td>\n",
       "      <td>3.239272</td>\n",
       "      <td>33.4</td>\n",
       "      <td>51.980229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.497337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476077</td>\n",
       "      <td>7.184111</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.264372</td>\n",
       "      <td>3.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>18.708888</td>\n",
       "      <td>399.487766</td>\n",
       "      <td>6.115159</td>\n",
       "      <td>36.2</td>\n",
       "      <td>51.611452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM    ZN     INDUS  CHAS       NOX        RM   AGE       DIS  RAD  \\\n",
       "0  0.218960  18.0  2.629288   0.0  0.869420  6.875396  65.2  4.347275  1.0   \n",
       "1  0.141576   0.0  7.315612   0.0  0.549711  6.499894  78.9  5.315684  2.0   \n",
       "2  0.380457   0.0  7.340354   0.0  0.697928  7.263489  61.1  5.356935  2.0   \n",
       "3  0.313563   0.0  2.562407   0.0  0.599629  7.209732  45.8  6.103983  3.0   \n",
       "4  0.330105   0.0  2.497337   0.0  0.476077  7.184111  54.2  6.264372  3.0   \n",
       "\n",
       "     TAX    PTRATIO           B     LSTAT  MEDV        RM2  \n",
       "0  307.0  15.534711  397.462329  5.715647  24.0  47.271066  \n",
       "1  255.0  17.914131  397.012611  9.338417  21.6  42.248623  \n",
       "2  243.0  17.919989  396.628236  4.142473  34.7  52.758270  \n",
       "3  226.0  18.979527  398.564784  3.239272  33.4  51.980229  \n",
       "4  234.0  18.708888  399.487766  6.115159  36.2  51.611452  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['RM2'] = boston['RM']**2\n",
    "\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.539\n",
      "Model:                            OLS   Adj. R-squared:                  0.538\n",
      "Method:                 Least Squares   F-statistic:                     294.5\n",
      "Date:                Fri, 11 Mar 2022   Prob (F-statistic):           2.19e-85\n",
      "Time:                        15:19:39   Log-Likelihood:                -1644.1\n",
      "No. Observations:                 506   AIC:                             3294.\n",
      "Df Residuals:                     503   BIC:                             3307.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     71.7363     12.865      5.576      0.000      46.460      97.013\n",
      "RM           -23.7896      3.867     -6.152      0.000     -31.387     -16.192\n",
      "RM2            2.4691      0.290      8.513      0.000       1.899       3.039\n",
      "==============================================================================\n",
      "Omnibus:                       83.768   Durbin-Watson:                   0.715\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              926.703\n",
      "Skew:                           0.270   Prob(JB):                    5.87e-202\n",
      "Kurtosis:                       9.608   Cond. No.                     2.13e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.13e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "olsB = smf.ols(\"MEDV ~ RM + RM2\", data=boston, missing=\"drop\").fit()\n",
    "print(olsB.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='MEDV'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAABS20lEQVR4nO29eZwc9Xnn/37q6HvuQweSkAQCcdjYIBO8JkQGO4shASdrO7CbrHd/SWB3/Qs4iRMcJ/bGdpKFxL9ssHMsxNldHCdgh9iBeA2xF1kGJ2AQlw1GIDEI3XMffXdX1ff3R1W3enq65+ye6Zn5vnkNPV3TXfVUj+apbz3H5xGlFBqNRqNZPxgrbYBGo9Folhft+DUajWadoR2/RqPRrDO049doNJp1hnb8Go1Gs86wVtqA+dDb26u2b9++0mZoNBrNquLZZ58dUUr1VW9fFY5/+/btHDhwYKXN0Gg0mlWFiLxZa7sO9Wg0Gs06o2mOX0TOF5EXKr6mROSjItItIt8WkUPBY1ezbNBoNBrNTJrm+JVSryql3qaUehtwGZABvg58HHhMKbULeCx4rtFoNJplYrlCPdcAryul3gRuBO4Ltt8HvH+ZbNBoNBoNy+f4bwLuD77foJQ6BRA89td6g4jcIiIHROTA8PDwMpmp0Wg0a5+mO34RCQE3AH+3kPcppe5VSu1RSu3p65tRjaTRaDSaRbIcK/73Ac8ppQaD54MisgkgeBxaBhs0Go1GE7Acjv9mzoR5AB4GPhx8/2HgoWWwQaPRaDQBTXX8IhID3gt8rWLzncB7ReRQ8LM7m2mDRqPRrFZyRbcp+21q565SKgP0VG0bxa/y0Wg0Gk0dJjNFXKWI2GbD9607dzUajabFyBVdxjKFpu1fO36NRqNpIRzXY3AqRzPH4mrHr9FoNC2CUoqhZB7Xa+4sdO34NRqNpkUYTRealtCtRDt+jUajaQGSuSJT2eKyHEs7fo1Go1lhckWXkVTzkrnVaMev0Wg0K4jrKYam8k1N5lajHb9Go9GsEH4yN4fject6XO34NRqNZoUYSxfIFpqfzK1GO36NRqNZAZK5IpPLlMytRjt+jUajWWaWO5lbjXb8Go1Gs4ysRDK3Gu34NRqNZplQSjE4tfzJ3Gq049doNJplYiS1PJ25c6Edv0aj0SwDE5kCydzKJHOr0Y5fo9FomkwyV2QsvXLJ3Gq049doNJomkik4K1rBUwvt+DUajaZJ5B13xSt4aqEdv0aj0TQBx/UYnMzjtZjTB+34NRqNpuF4nuJ0C5Rt1kM7fo1Go2kww6k8Bac1nT402fGLSKeIPCgiB0XkFRF5p4h0i8i3ReRQ8NjVTBs0Go1mORlN5UnnnZU2Y1aaveK/G3hUKbUbuAR4Bfg48JhSahfwWPBco9FoVj1TKyi8thCa5vhFpB24CvgrAKVUQSk1AdwI3Be87D7g/c2yQaPRaJaLbMFltMXKNuvRzBX/TmAY+F8i8ryIfFFE4sAGpdQpgOCxv4k2aDQaTdPJOy6DU7mWK9usRzMdvwVcCvyFUurtQJoFhHVE5BYROSAiB4aHh5tlo0aj0SyJYguXbdajmY7/OHBcKfX94PmD+BeCQRHZBBA8DtV6s1LqXqXUHqXUnr6+viaaqdFoNIvDcT1OTzanbDNXdPnct15lMtP4nEHTHL9S6jRwTETODzZdA/wIeBj4cLDtw8BDzbJBo9FomoXrKU5N5ii6jXf6k9kiH/u7H/C33z/KL33pGZwGH8Nq6N5m8ivA34hICBgA/iP+xearIvKLwFHgg022QaPRaBpKSVe/GU5/cCrHx//+h7w5lkGA69+yCcts7Bq9qY5fKfUCsKfGj65p5nE1Go2mmQwn803R1X9jJM0df/8DRlIFbFP47I0Xc9Pl2xp+nGav+DUajWZNMZrKk2pCg9aLxyf45D+8TCrvEA+ZfObGi3j37g0NPw5ox6/RaDTzZjLTnAat7742zB988xWKrqI7HuLOn30L5/YnGn6cEtrxazQazTxI5x1G0/mG7/drz53gz75zGAVs7Ypy1795Kxs7Ig0/TiXa8Ws0Gs0c5IouQ8nGOn1PKe59fICvHjgOwEWb2/m9919MR9Ru6HFqoR2/RqPRzELB8RrelVtwPO569CDfedVvTn3XuT389nUXELHNhh1jNrTj12g0mjo4ru/0Xa9xTj+ZK/Kph17mxeOTANz4ts38v+8+F9OQhh1jLrTj12g0mhqUhqk0slb/9FSO3wpq9AFu+fEd/Nw7tiKyfE4ftOPXaDSaGSilGEzmGjpM5bXBJJ/4+kuMpf0a/d/817u55oKV0ajUjl+j0WiqGE7myRYa16D15OujfPb//Ihc0SMRtvjsjRdxydbOhu1/oWjHr9FoNBWMNLhB66EXTvCFfYfxFGxoD3Pnz76Fs3viDdv/YtCOX6PRaALG0wWmGtSg5Xp+uebfPeuXa56/oY3f/5mL6Y6HGrL/paAdv0aj0eB35Y5nGjNBK1t0+YNvvsI/Hx4F4F3n9PCJ6y8gukzlmnOhHb9Go1n3TOWKDevKHUsX+O2vv8Srg0kAfvbSs/jPP3HOspZrzoV2/BqNZl2TzBUZaVBX7sBwik98/SWGknkMgY+8+1x+5u1nNWTfjUQ7fo1Gs25J5R2GG+T0nxoY5bPfeIVs0SViG3zqpy7kip09Ddl3o9GOX6PRrEtSeYehqVxD9vW1507w5/v9yp2+RJg/+JmLOaeJ6ppLRTt+jUaz7kg3aKXveoo/3XeYh148CfiVO7/3/ovoSYSXvO9moh2/RqNZV2QKDkPJ/JJF11I5h09/40c8++Y4AD++q5ffet/uZRNaWwra8Ws0mnVDtuAyOLV0p39iPMtv/8NLHA00d/7dj23jP75rO8Yya+4sFu34NRrNuiBbcDndAHnl54+O8+l//BFTOQfbFH79J8/nJy9szojEZqEdv0ajWfPkim5DNPX/8cWTfH7fYVxP0Rm1+cyNF3HxWR0NsnL50I5fo9GsaXJFl9OTObwlOH3H9fjz/a/zDy/4SdydfXF+7/0Xs7G9uSMSm0VTHb+IHAGSgAs4Sqk9ItINfAXYDhwBPqSUGm+mHeuJ/QeHuOfxAY6NZ9jaFePWq3ayd/fKSL/Ol6XYvBznu1KfabOPuxzn9fn/+xpf/N4bpAsu8ZDJL125g9vec96i7Kre/s6d3Tzy0mkGRtIA7OiJ8fH3XTDtHJbq9J8eGOPL33+T1waTFFx/H//qHH9a1g+PT/KHj7zKqaksm9qj3PSOrVy+s7v8vgeeOTbtZ8CMbaXXLzfSyHFiM3buO/49SqmRim1/CIwppe4UkY8DXUqpO2bbz549e9SBAweaZudaYf/BIT718MvYphC1TbJFl6Kr+MwNF7Ws81+Kzctxviv1mTb7uMtxXp//v69x977DGAKGgKf8r9uvPreu869n1wcuPYsHnztR3j6SyjOUzCNQlkLwFHTGbD73gUvYu7ufvONyamJpTv9z336ViUwRJ5jAlQibfOJ9F2CIcPe+Q1iGELENckUPx1PcfvUugBk/K6l9JsLWjNfP5vw7Y6ElibqJyLNKqT3V241F73Hx3AjcF3x/H/D+FbBhTXLP4wPYphALWYj4j7Yp3PP4wEqbVpel2Lwc57tSn2mzj7sc5/XF772BIWAZBoYYwaO/faF2ffF7b0zbnsw5/oUEMA3D/xIhlXe45/EB8s7Swzv3PD7AaKqA4ykE2NgepisW4qsHjvPAM8ewDP8iJPiPliE88Myxmj9LFxzSeafm61eCZjt+BXxLRJ4VkVuCbRuUUqcAgseaywsRuUVEDojIgeHh4SabuTY4Np6Zof4XtU2Oj2dWyKK5WYrNy3G+K/WZNvu4y3Fe6YJLtS6ZIf72hdqVLrjTthdK4xAr/LqI31B1bCzN6cnFz8lVSvHlp97kjdE0CrAMYWtXlPaITcQ2OD2V5dRUlog93X3O9jPPUzPsKb1+JWi243+XUupS4H3AR0Tkqvm+USl1r1Jqj1JqT19fX/MsXENs7YqRLU7/o8oWXbZ0xVbIorlZis3Lcb4r9Zk2+7jLcV7xkEm17/WUv32hdsVD5rTtITNwXRUXFqX8C0tfW2TRTj9bcPn0N37E//znI+VtlgFucOeQK3psbI+yqT1Krjh9LONsPzMMmaHOWXr9StBUx6+UOhk8DgFfBy4HBkVkE0DwONRMG9YTt161k6KryBQclPIfi67i1qt2rrRpdVmKzctxviv1mTb7uM3Y//6DQ9x871Ncedc+br73Ka7Z3YenwPE8POUFj/BLV+5YsF2/dOWOadvbIpafOwBcz8P1/P3HQhY/t2frnLY+PTDGr33lRW7+y6f4ta+8yNMDY5ycyPIr9z/P46/5KcmwJZgCjqcYnMwxnsnjeIqb3rGVm96xFcdTZIsuCv9xtp/FQxbxsFXz9fWwDKNp+v1NS+6KSBwwlFLJ4PtvA58BrgFGK5K73Uqp35xtXzq5O39KlQ/HxzNsWWVVPYuxeTnOd6U+02Yft5H7r5eUvWxbB48dHF5UVU+1XdXbK6t6lFJs6Yxyy1XnzFkp8/TA2IzkazrvkCt6ZIK7is6oTV8iRLrgMp4pUHA8IrbJJ6+/cEblzumpLBvrVPVU/gyo+/pqOqI2XbEQxhI1/Osld5vp+Hfir/LBLxv9W6XU74tID/BVYBtwFPigUmpstn1px6/RtDY33/sUQ8kcsdCZCvFMwaG/LcL9t1zR1GOXErnzDe/82ldeZDSdJ2qbKKUYzxQZSfuTtzqjNiLQmwghFXEkhSKZc/jbX27uucRCFl1xm7DVmJV+PcfftDp+pdQAcEmN7aP4q36NRrNGODaeoTNqT9u2HEnwUkfuQmL6p6aytEcsPKU4PXVmsLplCH/x85dy1yOvli8MZ47T3Hh8yDLoiYeJzpL/aCQrUc6p0WjWGCuRBC81Zy00kbupPUoy53B0LFt2+rGQyYWb2tnQHpk1ft9obNOgvz3Clq7Ysjl90JINGo1mCZTi7oeGkiRzDl0xm95EuBzjb1YSfCmCa5ds6eDFpybKlaCdUb9M899evg2Ay3d2czu75h2PXwy2adAZs2mL2HO/uAlox6/RaBZFZUJ3Y3sE28wzli7iuB67NrQ3LQmezi9OT9/1FH/95Jt86ak3AT+00x6x2NYdn+HYL9/Z3RQ5BUOEzphNR9RGVlDCWTt+jUazIEqr/OeOjiMCG9oiSEjoTUSIhaymJnSnFjkYfTJb5L998xWePuLLgl28uZ3/+tMXLtukLBGhLWLRFQvNqOdfCbTj12g086Zyle96HoYIJyf97tP2qN3UhO54usB4prDg9702mOR3H/4Rp4P5uj/z9rP4Tz+xE9tcnhRnLGTRHQ8Rslonpaodv0ajmTeVWjphyyTveLie4uhYhljIpC1isaO38UPGR1J5prLFBb1HKcX/+eFpvrDvEEVXEbEMfv0nz+OaC5ZnaMpyV+osBO34NRrNvKks20yELdKFM2GXgusxnCrwby9vbGx8KJkjlXMW9J5c0eXuxw7xTy8PArClK8rv/vSF7Oxr/EWpGssw6IqvXOJ2PmjHr9Fo5s3Wrli5USuV90cPOoFOfcg0aI9aPDkwxm0NOJZSiqFknnR+YU7/2FiGT3/jRwwM+zr9P76rl9/41+fzoxNT/Om+F5umh98qidv50DpBJ41G0/JUaunkHRcBLFPY1h1jZ1+Cnni4ITF+pRSDUwt3+vtfHeY//81zDAynMcS393d/+kJ+dGKKu/cdYjSdpz1iMZrOc/e+Qzw9MKtowLwQEdqjNlu7Y3TGQi3v9EGv+DWaNUmzpmvt3d3PZyDQzMmWq3rag/BPI5q2PE9xeipHrlhfvrmaguNxz+MDfP35EwD0JEJ88voLeOuWToBpGvlAWU/ogWeOLWnVnwhbdMVDy5YobhTa8Ws0a4zKypvOqM1QMsenHn6Zz0DDnH9JNO1TD7+MZQpKqYY0bbmB088HTr/WCMNqR31qMstnvvEKr55OAnDptk5++/oL6IqdmVxVkmmoZCl6+NGQSVcsRKRJ6pnNRjt+jWaNUVl5A345YabgcOcjrzT0LmD66n/pCp9F1+P0ZI5iMGSlUkWzMjxzO2fGFf7z4RHuevRVUnkHAX7hnWfzC1ecPaNWflN7tCH6O2HbpDsWaslKnYWgHb9Gs8aoJZjmuB5HRrNs91RD7gKqQ0mfvfHiJV1E8o7L4GQexzszwGS28Mzbz+7knscH+NpzfminK2bziesu4LKzu2ru/6Z3bOXufYfIFt1pM2/nq79jmwZd8RCJ8NpwmWvjLDQaTZnKypsSg8k8tmHMuAu45/GBBTvsRoeSsgVfYbN6Pm698MzxiQy33f8Crw76oZ1LtnTwO9dfMK0Lt1aI6ParF66/YxpCZyxEe8RaFUnb+aIdv0azxrj1qp186uGXyRScaUNRtnRGpr1usV229UJJi7mIpPIOw3V0d2qFZ8bSRcYzBUZSBT+0c8XZ/MI7p4d26oaIrt7FH//cDKX4mogIHVGbzqi95GEorYh2/BrNGqNW7D1kGmcGlAcstgJnIdr7s1UXTWaKjKbr6+5UhmdClnB6Ml8e1N4Vs/nt6y7g0hqhnaVU8IiIX6kTs7FWWaXOQtCOX6NZg5Qqb0qUwjPVdwGLqcCpFUqqdRGZLST0li0dTM4hwVCSR/5f/3KE14dTOIHu/p6zu/j4+3bTHQ/VfN9iK3jiYV9ErZU0dZrF2j9DjUbj3wXccBH9bREms0X62yJ85oaLFhWTn++g9sqQkIj/aBnwhX2H53T64DdxnU7meGM0jeMpTEO45cd3cOe/eUtdpw9+iGgiU+DYeIaBkRTHxjNMZAp1K3iiIZPNnVE2tEfWhdMHveLXaFqWRjdhVd8FLGU/8ynjrA4JKaWwTIMTE3PnFaayRf6/b7/GE4dGANjYHuF3rr+ACze3z/net2/t4AcnJjAERPwy0dG0x0+9pWPa66Ihk87o6i/NXAza8Ws0LUizm7CWynwuIpUhIaUURVeRLbhz1s6/cGyCP/jmK4ykfAnmd5/fx6++97x5l1I+f2yS7phNuuBSdD1s0yAeMnn+2CS/gJ+M7ozZq7b5qhFox6/RtCCNrJxZKUrVRal8EcswyM0xu9ZxPe578k3+9vtHUfhx+V9597lce/HGBZVSnprK0hUP0R0/8x6FYjCZY3NndF07/BLa8Ws0LchCKmdalb27+/lE0eV/PD7A6cnZa+ePj2f4/W8eLMsunLchwW9fdwFbuxdedVRdBioiFF2P7T1x7fQDmu74RcQEDgAnlFI/JSLdwFeA7cAR4ENKqfFm26HRrCbmWznTalTmJTa2R/jApVv44w/Vr51XSvHoS6f5wncOkyt6CPChPVv4f67csWjhs1IZaM5xiYcsCq6H69G0we+rkeVIYd8OvFLx/OPAY0qpXcBjwXONRlPBfCtnWolSXmJwKks8ZDI4lZtV+ngyU+R3//FH/NG3XiNX9OhNhPijD76VW3/inCWpXb7z3B5+57oL2NwRJZV3llTBtFaZdcUvIhuUUoOL3bmIbAGuB34f+LVg843A3uD7+4D9wB2LPYZGsxZptADacnDP4wNYBtimiVJq1sapp98Y4w//6VXG0n4C96rzevm195xXlndeDKVBKO0Rm7N74lx/yeYlnc9aZq5Qz4si8kPgfuDvlVKTC9z/nwC/CbRVbNuglDoFoJQ6JSI1/yWLyC3ALQDbtm1b4GE1mtVPo8ovl4ujY+lyBU+J6sapXNHlnscHeOiFkwDEQia3XX0u771ww6K1cIxAXqFjjcorNIO5HP9ZwHuAm4D/JiJP4l8EHlZKzdoGJyI/BQwppZ4Vkb0LNUwpdS9wL8CePXtmCnloNJqWYTJbpK8twmiqvvTxK6em+G+PHOT4uO86Lt7czm9dt5tNHQuTRi5hBJOvOqL2DBnmEs0aSLPamdXxK6Vc4J+AfxKREPA+/IvA3SLymFLq383y9ncBN4jIdUAEaBeRLwODIrIpWO1vAoYaciYajWbZUUoxkiqQzBW5aU9t6eMPXraF//3PR/jy99/EU2AZwn/4V9v5uXdsreuwZ2M+Dh9avxdiJZl3VY9SqiAiP8JP1F4GXDjH638L+C2AYMX/MaXUz4vIHwEfBu4MHh9alOUajabMSqxsXU8xWDEisaStUyl9/O7z+/jfTx7h0FAKgO09MT5x3QWc259Y8PFEhLaIr6cznwvGWuiFaBZzOn4R2Qb8HHAzEAceAG5USr0y6xvrcyfwVRH5ReAo8MFF7kej0bAyK9u84zI0lS9Pyypx+c5uLt/Zjesp/u7Z4/zp/sMUXVUu0/yP79qxKD2cRODwS9U+87nQrYVeiGYxV1XPv+DH+R8EblFKHVjMQZRS+/Grd1BKjQLXLGY/Gs1qZ6kr81rvX+6V7Wwa+uA3Y9316Ku8fHIKgE0dEe649vzy4POFkAhbdFYpZs73QrdaeyGWg7lW/L8FPK7q/YY1Gs28WejKvNrJv3NnNw8+d2LG+9P54owEabNWtmPpAhOZQs2feUrx9edP8MUn3iDv+HcCN1yymVuv2rlgIbRaDr/EfC909QbStHIvxHIxV3L3uyLyYRG5DdgdbH4F+LxS6ktNt06jWUMsZGVefZF4YyTFU2+MYgAR26SvLUxbxC43dmWLblNXtq6nGErmyAaDUKo5MZ7lD//pVX54wq/47m8L87GfPI8922cffFJNImLRGZ1dE3++IZzV2AuxXMwV6vn3wEfxm6+eAwS4FPgjEUE7f81KsFpL9OZyWJXnNZUtEg+bdEQjTGWLjKYLKAUe4HiKkxM5umIuyVyRnOORdzy6Yja9iXDDV7a5oh/PrxyEXqLWKv+6izfyn/aes6DB5PNx+CUWEsJZbb0Qy8Vcv5n/AvyMUupIxbZ9IvJv8JO82vFrlpXVXKI3m8OqPq/Tk/7qOmyZjKTyGAiGKDzllzM6ymMomccyBds0/KElU3mGpvJEbJNY2OSexweApX0uU7kio6lCzXj+0bEMn/unV3kpiOX3JcL8+k+ex+U75r/Kny2kUw8dwlk6c33a7VVOH4Bg29wTETSaBlNrqpNtStnJtTKz6e9Un1fYMkBgOJmn4HqI4A8WATxP4bgKBXjKHzQiCKb4dwQF1yMeMssXxf0HF94qo5Qf2hmpkcR1PcUDTx/ll790oOz0r3/LJv7qP+yZt9OPhy22dMXoX8TUq0ZOE1uvzLXin607d/YBlhpNE1jNJXqzxZx/56GXpp1XbyLMycksmYLrT5FSfklke9jEUZB3/QuBJeAiFD2PYCQtIjCSKrCzL7Go6p6C4zGUzFEIQjdPD4zxwDPHODWVpTMSIlVwODHh//lvaA/zsZ88n8tqDD2vRdg26YmHliyPrEM4S2Mux3+BiPygxnYB9H2VZtlZ7SV69RxWImRyeDiF6ylCpkFvIoxtCEVXUbngnsr7yVUBOqIWUzkX1/MvCiUcV6GU77Tr5RDq5UZSeYeRZB4vOOjTA2Pcve8QpkDR8Xh1KFl+7Y1v28wv//iOab+LeoRtk86oTXwBcX9N85jT8S+LFRrNPFmL8d39B4cYTRdwXIURzIg9Pp7BVf6qHvyQTsn/i/gx2vGMU3efJcddL4dQnRuplF6o5IFnjuG6HiPZIkXX36dlCNt74tx+za45z02POWxN5nL8UaXUQQARCSul8qUfiMgVwJvNNE6jqWYtlujd8/gA7cFquBTTL4VtPAXV4gRKgWUa4HkEvhiFfzHwKr6vl0OA6aWk7zy3p2YXbirn8OrgFNnime3dMZuuuE0qP/0CUU00ZNIVW3pIR9Mc5nL8f4tfvgnwZMX3AH9e9VyjWRbWWny3lLfwtWj8OP8rpyZxSv5WOLPcDzAMwTYMvKKf+AUwDcEOXioi9LdF6uYQwA8DHR1Nc3IiNy2Bq5TiiUMjfGHf4bLTD1sGG9vChIO7rHoD0yO27/AX2rClWV7mcvxS5/tazzUazSKolbewDKNcN1+rb95xPRzlV/YQ3BWcv6GtHPqqrnKpPoZSimTOl1KudPqDUzm+sO8w//L6KAC2KURsk46IRcg2yNYZmB6yDLrjoXnF+zUrz1y/JVXn+1rPNRrNIqjOW4ym87gVzrjGgp9iEAuyDcFD4XlwejLLrg3t5VV+ZTJXgFOTWb8JLMgX2Kawqz/Br33lRU5OZrAMg5FUgUIQ8rl8RzcfvWYXR0cz0xQ3Kwem26bv8HXSdnUx129ri4h8Hv/fXul7gudnNdUyjWYdEQ+ZHB5K4Xj+Kt42oD1sMJX3yk7fFMrduwAhUxARRAn9HTbbexLcf8sVwJlGt6LrMpYqkHdnrtNMgX2vDtMeMckWVbnzti1s8dH37GLv+X2ICBs7IjNGJ9qmQWfMLoemNKuLuRz/b1R8X63MuSilTo1Gc4ZKBw1nVvZKQbqoyglb8J1tW8Qi73g4rkfRU9iG0NcWJhG2ppVt3vbA86TzzrRqoBKW+DmAnOOXgU5kz+jvJMImZ/fEeHedHIppCJ3REO1Ra9GjEjUrz1wibfctlyEazUrTDA2gufZZqrYZTTkYhiDBit8DVBDOidgGrqfYtaGNTMGXRN7UGZ0WT88UnGllm+lCbacP4Cgwg5+Ufh62DPrbwkRsg5FUfsZ7LMOgPWrRHtFzbdcCc4m0PTzbz5VSNzTWHI1mZWiGBlDlPk2B54+O84tfeoZNbWHaoiGSga79xvYwBdfDNAQJwjmqwmnnHQ8UDAyn6E2ECJlSln6o7mUoXUgilkm6jpImQGXkxzaEbV1RRGRGxY5tGnTEbNrCeoW/lpgr1PNO4Bj+gPXvoyt5NGuUZgwzuefxAYquy9CUr6Ap+H9AxyfzWMkCZ3VGEIETEzksQ/z6fMPwk6sVGV2lfOfsuIoTEznO7Yvz8fddMKOXAeC5o+O4noe5ICetSBccTMMoV+zoTtu1zVy/1Y3Ae/HHLv5b4P8A9yulXm62YRrNcrJQDaD5hIUODSWZzBRx1BlJhVK83jSEkVSBDW0RTkxk8VQgzSBgGn4DVjFw/LYpmMGFAeXH56t7GfYfHOJjD75IwfGTwc48iu5CphAP+3cGp6fybO+J89H3nMN1b9ms6/DXOLPK4imlXKXUo0qpDwNXAIeB/SLyK8tinUazTGztipEtTg+N1NMAKoVwhpK5aWGhahXMTN6l6KlpYZsSIr6KZnvUDlb+QkfMJmqbdEVt9mzvoTcR4uzuKAaQL3oUHD8cNJzMzbDpzkdeYTxdmPUP2hC49qKNvPWsDrZ2RdneEydqW5giCDCczNETD2unvw6YUw9VRMIi8rPAl4GPAJ8HvtZswzSa5WQ2yeRqaklDF12X2x54nivv2sfN9z7F5//vazMuJNOaYhSEgsHhlmlw6bYuPveBS7hocweRINzUlwiTdzw8/NeGLQNPKZJ5d9pFpuB4DIykfbVOy6CW2/6J8/r421/6MX7z2vMZTOaI2AapvMNQMofrKixTyBTdRcs4a1YXszp+EbkP+Bd8aYZPK6XeoZT6rFLqxLJYp9EsEwvReD82niFaoUGTzBUZSRbIFNzyHcCf7X8d2xQCWf0ZuJ6iNxFiJJXj+HiWH54Y59YvP8uR0VR5H8OpPKNpf76tGIEUA0J33OaexwdQSjGeLnBiIovCv5g4nqI6pXtOb5yDp6e485FXeXpgjE3tUQqOx0SmgCkGpmkAfkJ4tcw20CyNuWL8vwCkgfOA2yqy+gIopVTdYSwiEgEeB8LBcR5USv1XEekGvgJsB44AH1JKjS/hHDSahjCbBlD1WETH9ehriwBwaiJb7qR9+eRUuerGNgVDDEzLl0goBKU0WzojtEVshlN5kjmHrphNMufgKY/RVJGwZZYboyYzRWxTKAZyzaWa/WNjaU5MZMua+T3xEKen8tP0HQQ/vFNwPdojFqPpPJ//ziE+cOlZ/MMLpyi4CtMIOnkV9LWFV81sA83SmKuOf2GjcaaTB65WSqVExAa+JyKPAD8LPKaUulNEPg58HLhjCcfRaIDG1uFX7isRMhlNF2iP2nRGbVzPYyjpr8Rdz5vWFaug/LzgKs7uiTKczJMunFG4nMo5fGjPVp4cGCvr54ympzBFUPhTt9oifrzfMGRazb5SilTeoTcRoeB4HB3N8Bfffd13+gECWMEdQmf0zH7aoya5osszRyb4zA0X+U1eBYeIZUwb3r5aZhtoFk/TarWUr/yUCp7awZcCbgT2BtvvA/ajHb9miTSyDr96X4eHU4FWvnB6MufX3AuMZ4plmYOaiF+9EwsZ5Zp62/CTxnfvO0zUFrb3JAA/hu+4CjEoa+Vkiy47emJkih6ZgkPINEgHuYeffusmvrDvMA+/eBI3uNvY2RvHNgwmcwU2tkc5MpqiOxHGNoxy01VpRb93dz+fv+nt5fOM2uaseQ3N2qKpRboiYgLPAucCf6aU+r6IbFBKnQJQSp0SkZp/lSJyC3ALwLZt25pppmYNMFsdfunn870TqN6X71gVQ8k8IcvANATP87VtTJneDFWJAMfHs+WLQ8j03wtQcFxSecXB00nClkEsZDKRLYLn1+yXnPAnr7+QguPxF999nVOTWfrbImztivLfHztEKu8PYtncGeE/XXUO7zq3p9xkFbFNPvrAC4ym84StM1mGykqltTjbQDM/pHqQclMOItIJfB34FeB7SqnOip+NK6VmHdi5Z88edeCAlgbS1OfKu/aVNe1LKKU4PZklFraxTcFxPQan8hQ9j119CT7+vgtqOrnLPvstckWPguv5K3FPlevjS0ldz1PlIei1yjVLnN0d5c0xfz5tyfG7wXvBd/Kl7G88ZJIterRFLHb1t/HLV+7g4q0dpHJ+pdF3XxvhL58Y4NSkX84ZD5v8whVn8/63ncULRyd44JljDCZzbOuK8Z/3ngMwbUVfT7JZs3YRkWeVUnuqty9LW55SakJE9gPXAoMisilY7W8CdO2YZsnUm8VbcBUdpu9sT03mEQFThCNjmZqhoP0Hh0jlXTylMMXvli16ZxQyc0V3RllmPaygScsQP4HqeB6mYZZ19g2Bs7r8HEDOcfEU3PPzl/ET5/cxlXWYyBZI5RxeOjHJ//juAD86NVV+30+/dTMf/ldn0xkL8fTAGF/4zmHCltATDzGSzvvndsNFfOaGi/SKXjODpjl+EekDioHTjwLvAe4CHgY+DNwZPD7ULBs064d6s3hDlkHUNnljJO3Pqg0SqK6nyqWLldr1zx0dRwVdtEr8MkoTwXPPiJrV0sevhV/J49ETDzGcKuAp8NSZsYq98RBtEV/aWCnFZLbIO3Z0c3w8S9H1+MaLJ/mr7x1hsmIO7rvO6eGXr9rJtm4/XBMLWXz9hRNEbKNmmOv+W67Qjl4zg2au+DcB9wVxfgP4qlLqGyLyJPBVEflF4CjwwSbaoGkRmqF8WUm9ePU9jw8wlMyVRdDgTPNUKdFZmcwtul45dFMaWB6xDDxPISKYIv4dQOC8Z7sIuApsETZ2RCm6Hsm8i+P5Cd/2iMmGjjNiaJmCQ19bmMGpHKOpPJ/7p9f4/pGx8s9DppCI2Pz0WzezrTtGPOwPMQ9bJicmsguSm9BomlnV8wPg7TW2jwLXNOu4JZrtaDTzpxnKl7WoV4f/qYdfxhQ/ISuAh6I3ESknOkvJXMedLq/gDzUX2qM245ki7RGT0XSxvGIvMZvzL3UBt0dD/MnPXVS+uyjdnRQdl8FknqKr2ObBZ//xR/zLwGg5IWwbQm8iRCJskXM8/u7Z4/zsZVsIWWcqreuFuXRZpqYeS6nTb1nmq6WiWR5qSRwspUN0/8Ehbr73qbI8wmy/11JH7o7eOK7yQzedUYvBZI4joxkmMgUODSWJ2iYjqbwvjcx0Zz6eKdIXtxnPOH5pZFUr7mxCmEVPzegC3ru7n0//9IUYwPGJHJ6niIdNjo5n+M5rw+QdP2nclwixvSdGW8TGDIawDCVz05w+LExuQqOBZUruLjfNkNjVLJ6FKl/OxmLuHkp3AvsPDnHnI69waDiFbRhs6YxQcD2SOQfbzFNwPSxDMIJwjgRyyLGwRVs0xOlkwa/ND5K14F8cKp9XE7aM8jjEkv1/vv91jo6lmcoWiYUMskVFMufX+QvQ3xamvy3CRLaAGVQCGSJ1m6t0WaZmoaxJx99IR6OpzUJCaY0MRSzlor53dz/3PD7Adk9Ns6UrZjOWLpbr80UE2zDY3BnBNIT+tgjHxjN0xaxyqKfyjmC2Hi7HU+XP6rXBKZI5h7awiWUaZIrT32gaoDwYyxS48W2beeSl0xRdD8uYu7lqNrkJjaaaNRnqWYjErmbhLDSU1shQRLVAGizsol7r/b2JMG1hk+3dMdxAO39TRxjTkLKdiZBZDvVErDONWLbh1+rXIhC04nceeomTExlSuSKOpxjNOAwGkg/gl2daQcWRafjPH315kA9etmVeonEazUJZk45fxzyby0Jj9gtRvpyLpV7U671/14Z2Hv3Vn+Cv/v07ePu2LjzFNDvLjWFBAqAkk2AYBiOpArVQ+M6cQKAt704PCUVtoxwmEgFRgAgb26PYpvDkwBj333IFT9xxtS7L1DSUNRnq0THP5rKYUFqjQhH16vXne1Gf6/317EzmHc7qjDCSKlR03foln4V6mg2A4yqGU4Vpmj4l1cytXTHGMwWGUwUQwTYNehNh2qN+Xb8OTWqaxZp0/KBjns1kJcsHa13U37mzm3seH+B3HnqpZr6hOh/xgUvP4smBsQUtCkrnvLPPF1UbGE6VJR0Aim7tweYenCnNNAUVJAhs06DoeURDFrs3hChW5R10aFLTTNas49c0j6WuuhdKteN+585uwA+ljKfz/PVTb9IetTEFDhwZ5cmBUT8iU6Pa5vh4licHRtnSGeH33v+WssOfK1l961U7+diDL3JiIosb9AM4rqInbhEypay+WYvSCl95ikTYIlVwKLoeJydy7OiJcd1bNvHgcyfqfp66J0XTaJZFpG2paJG21qPkjBoRSpvNsVWWb0Ztk9F0nqFkgb5EiLBlcGw8i6cIum4X9m/ZCGSTlfIreXoTIUKmwanJHAVXYRnCuX1xrnvLJr75w1O8NpSa1uBlCGxIhMgWXSZytR1/bzxEyDIYzxQoOB5h2yQWMukIdPJLTr7eXUj1+WuhNc1CqCfSph2/ZkWZy7G9708e542RNK5SGPgNUfVq5hfDbF23pb4sVeO1tjH9bqLWNccU2LWhrVyHr5Ti0FCKLV3RaWGdTMGhvy0yrd6/xM33PjUjrDbb6zWaSlZUnVOjqUdlhVAyVywrVd72wPP80pU7eG0ohenXRZKfpV5+scx2Dan+WeXzYh1bLAERf1Siq+DYWKY83apUTTRbOWr13c9rg1Ns6ojWfb1Gsxi049cAyx9HLh3v6SNjhE2hLeLr4Yj4csaZgsuf7X8dU/wB484quDMFcEpTzwMKrseJ8Sy9bS62abKjxy8ndVzFSCpfFo/b3h2r2ZWcyruMpPLl+b6gE7+apaMdfwXrNYnWKBG1+Xx++w8OcdejB3ltKIVtCpbhr46HknksUzDFwPMgbAl5x8NA4c0iidDK2KYQMg3yjkc67/L5m94KwMcefJGJTJHSDBbHVYymC9z16MEZXcndcb+rOB62liWRrlkfaMcfsFwKkq3IUrWNqjVwNrSHa35+5Y7fqRym+PIERc9DAo38oqtw3NKgE8E0BMdTdEdtv9Z9FWEawuaOaLkmfzJbLH8OfYkwqZyDqxQh0x90bhrCwEiaXf2JafvpiYcpur7Qm+5J0TQK7fgD1rOw21K0jSrlG8zAgZ+azLO5M4JtCnc9erB8FzCVLWIaZ+raRfwB5qb49e6KM3H0vOs/E1h1Th9gS6fv9GFmaCaZdzi3PzFjTGTptdX1/Lv623QiV9NQ1qRkw2JYqgbMamYpMgilC6br+U7cEEEEhpN5HNfjtaFUWdMnnXcYzzjT9O4dz+98ratnv7RTWzHyjltXLqTe572jJ6alRjTLgl7xB6ylYRYLzVUspSGrdLcQMg0cV/maM+InNQen8tPuoirLIitZrc69HiHTT05PZoszOosTIZN0weXkZA7bFDa0hbFMg6Kr+OT1FwJaakTj08yco3b8AcvdjdosFqtXv1hto9IFszcR5uRkFjxQqPKIwi2dZ0oRJRAsK2nYL7DfqmUo1fNX1vVXXsw2dUTwFDxxx9XTfh+mwOHhNAA9MZupnMPxiSy7+hJ88voLpw1q0axvmp1z1I4/YK0Iuy02V7FYbaPSBdM2hc0dEQaTeRwXzumLo5SiWFGOE7EMckU/ebtanb5pCBHLIGQJmYLnn2MQqioNUbFMg/6g/LLy9zEwnMIUv5QnXXDZtaGNTMGhKx5edf/ONM2l2TlH7fgrWAvCbssxhGY20bO3b+2aITdQuotqj/pzYwm0blaj779wUzvgJ2NPT+XojYemVTOVwjalO8XK30fB9cqOv6TwuV7ySJqF0ey/Y+341xjNzlXUugV98LkTNbVjqu+iOiI2RcdjJF1siC3LjQDJXLHchVuqtqnULepvi0y7U6z8fZTyIEBZ1XO15pE0zaXZf8fa8a8xmpGrqFzhT2WLxMMmHVE/lDHXLWjlvNtPPfwyqVlULFsVS4KOXODoWCaYlCXceMlmYPY7xcrfR28ixImJHCjY2B7WVTuaujQ759g0xy8iW4EvARvxy7TvVUrdLSLdwFeA7cAR4ENKqfFm2bHeaHSuYv/BIT724Iuk8g6u58ez03mHsGXSFvFvRWtpzRwaSlJwPDzlYYjhly8qVVfjZiURgbBpkHM8BIiF/D+0kvJC2DaxPY+so3x1TqVoj1g8+NwJ3rqlc87cSeXv49y+OCJCKu/MuDvQaEo0O+fYNHVOEdkEbFJKPScibcCzwPuB/wCMKaXuFJGPA11KqTtm25dW52wec5WMXfvfv8vh4TRmUJ+fdzwUfqJ214Y2AIaTOTIFl5BlkMw5xEIGqZyLp9SqSOLG7OmDz0sVR6ZQvrU+OpYJRin6Q1SUgp6EzfaehG6u0rQs9dQ5m9bApZQ6pZR6Lvg+CbwCnAXcCNwXvOw+/IuBZgWYz9D0w8MpXE9RcD0KrlcuW8w5fkXLcDLHcKpAPGySyTt4SjGRcVDUb8pqJUyBTNE/LwM/ju8qXyhOBCxTGEnly+diGUa5SW0yU9SJWc2qZFk6d0VkO/B24PvABqXUKfAvDkDNexcRuUVEDojIgeHh4eUwc90x19D0/QeHcLwzUgqe8mN2pelWk9kimYJLf1uI3kSEoqf8wSbBa1tdUDNkCh5gGhC2DcK2ScQ2CZsGYcugMxaivy1CzvEwAtVQ0/AvfSKQdz2dmNWsSpru+EUkAfw98FGl1NR836eUulcptUcptaevr695Bq5j5pKpuOfxAUJm4OjwnV2J3RvaeOKOq2mP2vTEw4BfqaL8kbK+41+Ok6hBdWdwPTzlx+zDgd2V29MFl2TOAeC8/gT9bWF/lKOnUErhegrLMHRiVrMqaarjFxEb3+n/jVLqa8HmwSD+X8oDDNV7v6a5zKXRc2w8w6aOCFawyi05RxG449rdM/bR1xb2Hf98PW8TMIBt3VFsQ+b8x+14ELYMOmI2SvkO3/U8ikGfwcZAZfTEeIbRdAHHVRQ9j6LrYYjwkb3n6MSsZlXSNMcvvvTgXwGvKKX+uOJHDwMfDr7/MPBQs2xYy+w/OMTN9z7FlXft4+Z7n5oWl58vt161c1ZRsK1dMSzTCEYFmtim37V6/oa2ssOr3EcibNGTsFdUO98Djo1lAV8PfzYMoC9uY5smPQl/WHshyEb3JkK0R0O4niJT9HADHaJSuOu6izdw23vOa+7JaDRNoplVPVcCTwA/xP9bAfgEfpz/q8A24CjwQaXU2Gz7WkpVT2mQ9lpiqQO4Kyt5EiGzXF5YXTJW6ziT2SJ9iTDJvMPWQIDskZdOMzDia9Ds6IlxeDgdDDI/I9GwVMKmBFLNcyP4Iauc4855EbIN4VeuPrfceTyUzNMeMckU/GS266nygPWwaSCCP/9XhHt+/rKWX/Gv1+FCGp91O2z9+HgGpfCTdraftAtb5txvbGGWMoB7oReNyq7UeMhkNF2gPWoTtU1GUnmGUwU6Iha5okve9bAMg6LjYRqB43caU7jfEbGYDGLuc2EaQlfUYjRdnNdFpy8R4o8+cAl7d/fPKF+ttN82BFedGfa+e0OCR3/1JxZxNsvDUhcImtXPspdztgpKQdH1SOaKjCTznBjP8sZImpMTWUZTeVJ5B8dtwa6iWVjK7IC5Knmq2bu7n/tvuYIn7riarniY9qhdfq+f/FSMZ4o4nsIyBNfz8PCHkTfK6VuGMJlzyiWX9ZDgyzaF3kR41lxD5WuTOad8/umCi+Mp8lXlqwBFT027gzg4mOKyz35r0aG2ZrPQ37Vm/bAuJRuUUuSKLrmiC1lfN8Y0BNs0CFnBl+l/GUbrhYmWouOxFPGn0ntVUA2Td1xK18xyOK3Bd5Aifq29Mvwaek8pDE+VJRQqKW2yDWE0XfC1cTwP15tZYeQPdTcwDHBcj+PjGfYfHOLkZA7LCGL5au4KoVzRa9kxncsh2KdZnaxLx18L11O4XnAxqMAyDGyr4qIQ1HivZN5gKToepYuG4ypGUnlfMdIQtnfPvGh4QeNW3vEoOB79bWFGUnkiQajMDhwrBBUxrqLR905K+WMYQ4b/OzqrM8pIKo9bcKdp4Vc69s5YiJBlEA9bnJzIYVr+xd4J4vWli3zJbssw2NIVK6+QlSflxHDR9XBmSRSkCy5HxzKYhnDnI6+0lONfS8OFNI1lzYd6lorjeWQLLlNZP1R0csIPFR0byzA4lWMsXWAqVyRbcCkuU8ho7+5+PnDpWQwn87xyOslwMs8HLj1rXk7n1qt2MpktcmIiS8EJ9HMcj+FUnodfOMHgVI7j4xmOjKQ5MnomJJbMFfnQZVspuops0cXxvHKZJ/iD0pd69rUupaWQDIEwmmUKO3rjbGj3ewdMAyxj+nuPjWdxXI+2iM3mTr8c1VP+cJi2sH/Rcj0v+FK0RSxuvWonh4aSeK5H3vXIFV0Kjjur0y9hiuB5ikPDqZYK+cxVtaVZv+gV/yIpul5NRy8iWMGK0jT8701Tyl2fpgiWubTr7f6DQzz43An62sJsC1b8lYJhKkhAup7CU8pftQY16OdvaqMzajOVLeIpf9XeHQ9hiPA/v3eEi8/qqHlMpRS9bSHetqWTJw6PkMrPL9E6X0oNYrUiRabhyyjc/u5zytU3O3oTpPMOBcerWe1zbCzLth6hLWLTFrHJFBxsQxARDg0lKXpgibCrP1HuSUjmHJQItuGHkuarMyQifr5ApGGDMhrBWhkupGk8a76q59hYZtlW4gvBNPzB5IYhM7piS5TKCEsx9RK33f88o+k8kYoEb7bgYpsG7RGbU1NZNrVHuekdW7l8Z/eM/d78l0/RHrGQinVyKl9kOFWgOx4qv/f8TW089+Y4zxwZ58CbY4ykCjXPxRAwFCzlUmAaYAQfglMxfN0KLp47euM88tGrpp/HvU/x/LFx8kWvZvVOxDI4tz9RLkMVKFckVVe43HzvU7wxkmI0XcBA8JRXVhI1gvr9eudeEm3b1BEuj1zUaFqBelU9esW/QriewkXBIuTpT05maY9Y0wLbrudxYiLLWZ1R2iMWo+k8d+87xO3smuH8N7VHGU3ny5VBqbzD4FS+rEfz+nCKTz780jQHXKK/Lcz2njiHhpKkcqWBKoISxWyxnlJUqJYDFcDzQMn0qpn+RIi2qE3RVeVVeSW3XrWTX/zSgbolm65STGaLKM9jLF3AUzCaLtATD7GxIzptjsChoSSZkvT0AhdDliH0tYUxDSmPXFzp+vmVPr6mtdGOfxVS7bjTBYeTk3kARlJ5uuMh4iGLbNHlgWeOzXD8N71jK3fvO0Sm4K/RB5M53CC0cWIiN+21Icvgki0d7NnezZ6zu9jeE+PXv/oDEmGLZK4Y3LEIyoN66jwGfhy8XrzcCEI8JX8bMgARRjNFehJhPnn97vIwl0qtf9sUwqZBxjtz9awchB61TT542Rbu3ne4fEHxFAwHdy4b2iPlap5kzlcWDVmGLz2tZtpViSlgGMK27tiMBHuzB2XPxUofX9P6aMe/Cik57mzRxfU8Bqd8p28KOJ5iaCpPf7s/UOT0VLb8vlzR5WvPneAfXzzpa8/UccRhyyAW8i8qf/tLP0a4qmfg1FTW16z3/OMZ4idOSyGr0m5LTtjDrxCqh0epvFIImQY7+xKA35TWGQtNm+BVdF0mM0W8II9RvddypY/46ptf/N4bvgMv5Q+Cx1IjWjxkctsDz1N0/P6DyrscT/l3HaULhQDbumO0ByWtp6dy9LdFZsTPb773qaYOyp6LZg/q1qx+tONfhVy+s5vb2cUDzxzj5VOTmKZgeL5XM0TwUIylCygVImKb/MX+1/nhiUleG0zWDLXYphC2DNrCNrGQiWkI2aJLTzw8w+lHbJOOiM3rwylMQ8qSBkXlN3ApoDNiMpVzah7LgHJvhOupIIcRhIKUL/RWolopNJUrMpaZPq93tvh7wVWk8g62KYhh+LmeipX/GyNpTENwXIVpBOGmqn1M5RzCloGrFCHToD2oi6+cuVvNQuvnGx2W0fX7mrnQjr/BPD0wxgPPHJszwbpULt/ZzeU7u7n5L5/CFBhK5v0VfKAsWXQV2Uk/bHNkdPoffNgyiNomUdtExE+CZh2/PNMwCMo1FTe9Yyvg9zK0RSwSEQvbNDAkaNgKYiBn6ukVt129i0deOs1ENlnTbg//ziRk+iEV11NYgaRAPHxmnCP4YatMweXKu/ZxciJb08F7yl/ZF1yFHYSIPM+f/JXO+xefoqsIWwZgTEv024aU0xL1moxzjuc3kAFtcQul1Jx9Ewupn29GWEbX72vmQjv+BvL0wBh37zuEZcicCdal4HqKI6NpXhtMkc27TFWVVlb6R9sUdm9s4+KzOnjkh6fojoewDKPitYpkzuGj15zHA88c4/RUlo3BBWvv7n7ao9Y0BwKQKrh0xSxGUtO1cEpVMJ2xkF+CWceZFl3F5o4QlmlQdBUfuPQsvvrMUY5P5jk1mQ8uTAbJvD/kxZxlVe8fV5UHohtBCMg2/dCT6/rPq+Wn+xMhxrNFf3CMombYqxSqsi2D3pjN1p7EjLBOrdX6rVft5DcefJFjY5my2qdtnhnOXkkzwjLNHtStWf1ox99AHnjmGJYh5aRr6Y+uVoJ1vuSKLm+MpDk8lOLwcIrDQyleH05TmEUHR4DueIjP3HgR5/Ynyl2qr5xMMprOY1W0EeSKHhvbo+U7iGfeGOPvnj3OF75ziIdePFkz7LC1K8bzx8bLoZ4SCviz/a+TCPtTrLLKq5kYNQROT+W5dFsX79zZzZeeepOJTLE86zbv+N3CEctgKuuQKcxe+qSUH8M3xU8iF4Kh8LMxlXMwxXf6luEnnis7gEOmgUC5YWwyW5wR1qkeRD+SyvOxB1/k319xNrnA2ZZ+H66r+Oun3pwxnL0ZYRldv6+ZC+34G8ipqaDMsoKIbUxLsNbD9RSnJrMMjKQ5MpJmYCTNwHCaE+PZuuWK7YEqZsgyKARhk5Bl0BXzNfEv2NQ+7fWVSeGIbZAreuWQTixk8fzRcf70O4eDfYTqhh1KZZRu4CxL2MGFoOgqOmI2+an8jGpVXxjNKK+wv/nDU+VmsGpfnQvCT3MVV9qmlKUY8vMUhss73rQximHLwPV8WQdTghJTFL2JSN0wyZ2PvBJcsPzGPOXBRKbI//juAB5qmtaTp1RZDK7ys2xWWGbv7n7t6DV10Y6/gVSXWcKZFfWZ5y4nxrMcHctwbDzDm6MZ3hzLBI1m9V1cbyLEuf0Jzu1PsKu/jfM2+OMAf/2rP5hxzGzRpT8enrGPyqTw6aksmzqi/OKVO7j24o1YpsGXnnyTkGUQC1m+REUqT97xuO2B5/n8TW8vO5K9u/s5rz/BK6eTKAjq/8+skEOWgW2a9LeHORWUmRK8TsSXTwibwlAyx5HRNDD9zqGSettLCP7qvDcR4mRVKepslLIT/uB0X8qhNxEmXXA5OZlDDNjcFsEypW6Y5I3RjN+8VnEBUZ4iU3SxTf9iULZTzojBVaLDMpqVQDv+BlJaUacLDqZAOhjmsakjyq999UVOjGcZTuVn3UfYMtjeE2d7b4ydfQnO6YuzszdOZyw06zFrreJrcfnObt59QT/tEZtoULJZilM/fWSMSFDKOZEtYiCYht8nUL3yv+Pa3dz65WfLGjhK+SvktojNjt4Et161k3seH6Do+ivdrphNMucEUsdCf3vElwk2DAqzdFbP5vYN8cMiO/sSZAoOhiFYqLqJ2mocD2wT7q0YqLL/4BB3PXqQgZE0p6fy7OiJ8cnrL1zQ6lnEv/tQ3pmObD+kZMxYyeuwjGYl0I5/gTiu3wU6nMoznPS/BpN5BqdyDE7lmcgUZyQSnz4yc8BYPGyytSvGtu4YZ/eUvuJs6oiUpQvmQ/Uq3nemiv/+2GtseuZMVZFpCImwRXvULsf8YXpVSdgUCq5HJuX6DUqWgaf8qh/b9NUnKxOZ1128gW++NIjj+VUzbRGbkGWWHVelMy1dWMKm7/RL1Tsb2sMcHZs7FFYLyxDyjkum4DCZ9UMu+Xk6feHMhaPWxLFdgdRDplh/hzt74xwaSiFKlXWGPAVndUTIOR4TmSIquGPxFHTF7JoreR2W0Sw32vHjh1+mskUmK77GM0UmMgXG0kXGMgXGUgVG075jn29Dv+BXhJji69Bce9FGrjyvl61dMbpidsOknUuJ2VpVRZ/fd4hPRi/k2rdsrHm8yqqS/vYIJydyKJQfmw9GD7qGcGQ4jQuYRoqwaQQSESYf2XtGOK3earXk2GpNDrNMg90b23h9KEXBO5MM9WPsZ4alFIN+gVLSFRRiCKLEF1/Dz6dkivPTwFD4OQWlzjj2hVbY3HHtbn7jwRdJ5vxhPpbh51d+7/1vAfwcwBtBKe2uvjh3XLtbO3hNS7BmHb9Sir98YoBTEzlSeb8yJFNwSOcdUnmXdMEhmXNI5opzVoDUoy1i0ZcI098eZkNbhP72MBvbIzzwzDHSeYd4MM8W/Lj768NpPnL1uY08zWmUq4pCJqYI7VGTXNHlS0+9yfveuqnmeyqrSnwZY3hz1B9X6Sq/xh6lygJspgiugtFUkZ4EPDkwNue4xxL14tmfvN7X4amskDEN36F3RG1OTeXKiWP/P+iJhwhZJp+54SI/pOQpTk/mZpSRmsEAF0MoJ36VOqMEmikqrrxrH1u7Yrw2OMWmjug0m2ersNm7u58/+sAldcM02slrWpU16/hFhM9967VZyx5rYQh0RG26YiE6YjbdsRDd8RBdMZvetjA98RA9iTB9iXA5Rl7NvU8MzFC/nG91z1I4nczRFbUxA/lhmLs0sLqqpC1is7EjzOmpvC8lLdOHnJfCOh6KyUyR48b8yw7nimd/ro4TLYWKXjw+US7tHE4VuOGtG9m7u5/feeglOqM2Bdfz5bDFl81WQMQyidoGiOC4gbaQ+NIMTiDEU2qcSuVdRlJ5+gKhNZi7wqZkeykEVhprqJ2+ppVZs44f4LJtXWQKDhHbJBYyiYUs4mGTeNgiHrJoj1p+3Dti0x616IjaxMPWgmLstZhPdc9CmK0buBS7b4vY7OiJ+07cnH9p4Dt3dvNn+1/H8TzCpkFHzMY2TeIhA6WknHgt1biX6vJFIO96Cy47nC2ePdvPjo2ly07f79CFh39wmh29r5UvXqGgTNQ0BMHAMoWNHZGyYuaR0RRTWT/B7ClfpiFimeV5tN1xm7F0kXjYmneFzXw7b7VapqaVaJrjF5H/CfwUMKSUujjY1g18BdgOHAE+pJQab5YN999yxYro8S+00mY26nUD/7p5Hv/64o0kwlZ5dX/rVTv52IMvcmIiWw6XJMIWn7z+wpr7Lg10KVXc5B2PsXSRj+zdxpMDY+U7gYHhFPmiW55zq5Tyw0CGsaCyw8U4v5JjPRnITwh+h7AEIZ+79x3m9qvP5cHnTtAetRhJFvDEN7QtYk9z3J96+GU2dvhO/ZXTUxjiD2Uv0RMPU3RVTeG1eswnL6DVMjWtRjNHL/5v4NqqbR8HHlNK7QIeC56vOS7f2c3tV++iJx4mmXPoiYe5/erFyTZUdgOLCPGwRcQ2+PvnTtAWmZkgFgDlO2dU7XGG+w8OcfO9T3Hrl59laCpHJCiJvGBTO1u6ojw5MDZtbF9vIgQiGPgyzb4ip/CRvefM23GVnN9QMjfN+c01qrDkWKvL+UtPXU/x4HMn+MClZ7G9J0FHzB+00hGx2NGbKA9a2bu7n8/ccBH9bREms0XiIYueeKgsugbThdeeuONq7r/lijnP79h4ZtqdHcwMr1VeHEp3F7Yp5bCQRrPcNG3Fr5R6XES2V22+EdgbfH8fsB+4o1k2rCSlSpulcmoqS0fExgwagkqjHWvF7e95fID2qM3GigTlbKtPT/mdtycncmzu9FfIJadVHY8/ty+OiJDKO4uqNV+sJk0p+VxS4axOw/sTsGReSebSBaBUq//aUIqJbJENbeGybtA7d3Zz871PzfuuZD6dt1otU9NqLHeMf4NS6hSAUuqUiNT9ixKRW4BbALZt27ZM5rUWsZDF2d1xRtN5wsb0ztxasfX5OJhKBxwyDRzXvy0YTuZpi9jT9t3I+vLFOr+SY+2NhxiqMfqxJx5akBOtvPBt6YwwOJXn+ESWXX0JbrxkMw8+d2JBIZn5dN5qtUxNq9GyyV2l1L3AveDP3F1hcxrGXLLNlmGQiFi0Ryws0+C/7D1n3i39C1199ibCnJzMIoEwWqbgNE0uYL7OrzoP8M6d3Tz43AnaojYKxXDK1+MXfBmL0gjFLV2xeeUQqu882qMhMgWHrniYJwfGFnxXMp/OWy3LoGk1ltvxD4rIpmC1vwmYPcC7xphNtnnv7n7aIhaxitp/WFhL/0JXn6X49mAyhyh/Xmz1vhtVjVKSKj4xnsXxvLLGf2XiuVYStBS/f3JgjILjccHGCMOpPB3B0PThZI7xTJHhZJ5bv/ysX3abCNddrc9256FgUXcl9e6MKj+7trCv5T+ZLWpZBs2Ks9yO/2Hgw8CdweNDy3z8FaVatjkWssg7Lv/wwglu+rH64az5hlwWsvocTubKlTyg2NQRnVGH3uhqFAVQGuIiM+P11atxx1UMJXP82f7XuXRbF5+98eLps3cHp0jmXbrjdnkc42i6QNgyaY/aNVfrc915NCokU/3ZlS7CpXPQaFaSplX1iMj9wJPA+SJyXER+Ed/hv1dEDgHvDZ6vG05NZYnYJoYh2JZByDJIhC1OTDSusWvv7v5Zq1L27u7nA5eexXimSM5xMfClEE5N5oJJXmeqbRpZjXLP4wN0RG129bexe2M7u/rb6Ija0/ZVWSEzlS1ycjLrT9PyvGl2lc5x1wa/Cqk3EaEYlK8aCCOBEF6t1XpltZJSalp4a7afLeZ8dSWPplVpZlXPzXV+dE2zjtnKhG2Tbd0xRlN5Ivb0EsLlTvI98tLpQKRMyqWZpiGMpAplpctSiKJR1Sjz2VflanwklcfAvzMIm0bNeHvlPkvNWyKUm85qfbZz3RU1Simz1vk6rsdzR8fLEhE63KNZKVo2ubsWMA2/7r4tYhG2TD6y99wVT/LtPzjEa0Mpf1qV4WvNe8ofQFIAkrkiibDF8SCm36jQx3z2VZmj8OWbASXlAeyzXSj62sKcnMiVB6DMZ7VeCjX94PhEw7tqq883mStyYiKHpZu4NC1AMxu41iUifrfshvYI27pj9CbChC0/fFHdRNTfFik3GFVSarC68q593HzvU3M2OS2EUghCELzqhmbxa/pH0/nyarde6GOhNs4njFL5+RgiGCJs7jwj4VzrQlHaZyJs0ZOwMUSI2sasn21lI9mR0RR37zvMGyOpBTWWzUX1+Z4OOo83tEV06Eez4oiqNRS1xdizZ486cODAot67XJINdqBzkwhZ5YlMi6EyKVh5V1DLiS2GK+/ahylwcjKH46rpg9kNP7RiiHBPMJykFOuvDH0Ai7Kx1r7qvX6+n8NC9gnMkIYeGE5RcD1CpsHOvgTgN731t0XmrTo6n/MdSubZ2B6mPXpmoE6pyueJO65e0nE0mnqIyLNKqT3V23WoZ4lEbLMs7tYIFtvhOl9KIYjNHVGOjp0Jm0jwv5BpELWNadLC1ce9+d6n5m3jYstB51vGutAms+rYe8H1MCryAtC4rtpK20oXnEp0E5dmpdCOfxFUKmKGrMZGy5rd3l+Ko/sraYNiIIKzuSNaLoHsr5AlXoqNSy0HbcZkqurYe8g0yiv+Es1wyLqJS9NK6Bj/AihNqdrWHaMnEW640wffMVWPbmykI6qMo8cCCeqeeIi2iDXv8sX52jhXSWMzcxn1qI69t0ctPOUP1VlqCedszDe/o9EsB3rFPwe26XeYJsK+hEKzWY6VYa15uAspX5yvjbPdGayUVHF1CGl7T4Kb39E95/jIRh1bO3pNK6CTuzUwRIiFTdojNhG79pStZrIYZ9xMO2rF5+djY3UidSpbZDCZQykIWwaxkDlt2lWjkqoajcanXnJXO/4KIrZJImItuTJnLdCI6qLKfTiux4kJP7l5VmeE4xNZv1wzyC2ArnLRaBpNPce/7mP8lmHQGQuxtTvG5s4o7RF73Tt9aIzkQGVcuzTDd0tXlPZoiEjQ21CSVwBd5aLRLBfrMsZfCuW0he26A9PXO42qLirFta+8ax+d0TMTw/rawpwYz5J3PJRSuspFo1lG1pXjj4ZMEsGgdb2qn51GDw+p3l9bxKa3zSWdd7VUsUazzKx5xx+2DdojNvGwuSxVOWuFRlcX1dqfbZp8/qa3amev0Swza94T9rdF6IjZ2ukvkEbXnes6do2mdVjzVT0ajUazXtFVPRqNRqMBtOPXaDSadYd2/BqNRrPO0I5fo9Fo1hna8Ws0Gs06Qzt+jUajWWdox6/RaDTrDO34NRqNZp2xKhq4RGQYeHMZD9kLjCzj8RbLarBT29gYVoONsDrsXE82nq2U6qveuCoc/3IjIgdqdbu1GqvBTm1jY1gNNsLqsFPbqEM9Go1Gs+7Qjl+j0WjWGdrx1+belTZgnqwGO7WNjWE12Airw851b6OO8Ws0Gs06Q6/4NRqNZp2hHb9Go9GsM9a94xeRrSLyHRF5RUReFpHbg+3dIvJtETkUPHatoI0REXlaRF4MbPx0q9lYYaspIs+LyDda2MYjIvJDEXlBRA60op0i0ikiD4rIweDf5jtbyUYROT/4/EpfUyLy0VayMbDzV4O/mZdE5P7gb6nVbLw9sO9lEflosK2pNq57xw84wK8rpS4ArgA+IiIXAh8HHlNK7QIeC56vFHngaqXUJcDbgGtF5Apay8YStwOvVDxvRRsB3q2UeltFrXSr2Xk38KhSajdwCf5n2jI2KqVeDT6/twGXARng661ko4icBdwG7FFKXQyYwE0tZuPFwC8Dl+P/nn9KRHY13UallP6q+AIeAt4LvApsCrZtAl5dadsCW2LAc8CPtZqNwJbgH+nVwDeCbS1lY2DHEaC3alvL2Am0A28QFF+0oo1Vdv0k8M+tZiNwFnAM6AYs4BuBra1k4weBL1Y8/yTwm822Ua/4KxCR7cDbge8DG5RSpwCCxxWdCh6EUF4AhoBvK6VazkbgT/D/0XoV21rNRgAFfEtEnhWRW4JtrWTnTmAY+F9B2OyLIhJvMRsruQm4P/i+ZWxUSp0APgccBU4Bk0qpb7WSjcBLwFUi0iMiMeA6YGuzbdSOP0BEEsDfAx9VSk2ttD3VKKVc5d9WbwEuD24RWwYR+SlgSCn17ErbMg/epZS6FHgffmjvqpU2qAoLuBT4C6XU24E0Kx96qomIhIAbgL9baVuqCeLiNwI7gM1AXER+fmWtmo5S6hXgLuDbwKPAi/jh56aiHT8gIja+0/8bpdTXgs2DIrIp+Pkm/JX2iqOUmgD2A9fSWja+C7hBRI4ADwBXi8iXaS0bAVBKnQweh/Dj0pfTWnYeB44Hd3UAD+JfCFrJxhLvA55TSg0Gz1vJxvcAbyilhpVSReBrwL9qMRtRSv2VUupSpdRVwBhwqNk2rnvHLyIC/BXwilLqjyt+9DDw4eD7D+PH/lcEEekTkc7g+yj+P+iDtJCNSqnfUkptUUptx7/136eU+nlayEYAEYmLSFvpe/yY70u0kJ1KqdPAMRE5P9h0DfAjWsjGCm7mTJgHWsvGo8AVIhIL/s6vwU+St5KNiEh/8LgN+Fn8z7O5Nq5UUqNVvoAr8WO+PwBeCL6uA3rwE5WHgsfuFbTxrcDzgY0vAZ8KtreMjVX27uVMcrelbMSPn78YfL0M/HaL2vk24EDwO/8HoKsFbYwBo0BHxbZWs/HT+Iukl4C/BsItaOMT+Bf2F4FrluNz1JINGo1Gs85Y96EejUajWW9ox6/RaDTrDO34NRqNZp2hHb9Go9GsM7Tj12g0mnWGdvwajUazztCOX6PRaNYZ/z8yA+v2mTlaEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = boston['RM'] + boston['RM2']\n",
    "y = boston['MEDV']\n",
    "\n",
    "sns.regplot(x=x, y=y, order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjxD1XjXjl7M"
   },
   "source": [
    "The R-squared value is higher for the second model with RM-squared, which means the second model better explains the relationship and shows that there's a curvilinear relationship between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkO8pHRvjl7N"
   },
   "source": [
    "### 1.2 Training and testing\n",
    "\n",
    "Chances are, for the above problem you used all of your data to fit the regression line. In some circumstances this is a reasonable thing to do, but often this will result in overfitting. Let's redo the above results the ML way, using careful cross-validation.  Since you are now experts in cross-validation, and have written your own cross-validation algorithm from scratch, you can now take a shortcut and use the libraries that others have built for you.\n",
    "\n",
    "Using the [cross-validation functions](http://scikit-learn.org/stable/modules/cross_validation.html) from scikit-learn, use 5-fold cross-validation to fit the regression model (a) from 1.1, i.e. the linear fit of housing price on number of rooms per house. Each fold of cross-validation will give you one slope coefficient and one intercept coefficient.  Create a new scatterplot of housing price against rooms, and draw the five different regression lines in light blue, and the oroginal regression line from 1.1 in red (which was estimated using the full dataset). What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gnVo_0TDjl7O"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "# your code here\n",
    "\n",
    "X = np.array(boston['RM']).reshape(-1,1)\n",
    "y = np.array(boston['MEDV']).reshape(-1,1)\n",
    "LinearRegression().fit(X, y)\n",
    "\n",
    "\n",
    "scores = []\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    lm = LinearRegression().fit(X[train_index], y[train_index])\n",
    "    y_hat = lm.predict(X[test_index])\n",
    "    \n",
    "    plot_x = np.linspace(0,10,20)\n",
    "    plot_y = (lm.coef_[0]*plot_x) + lm.intercept_\n",
    "    plt.plot(plot_x, plot_y, 'lightblue')\n",
    "#     print(lm.intercept_)\n",
    "#     print(lm.coef_)\n",
    "\n",
    "\n",
    "plot_x = np.linspace(0,10,20)\n",
    "plot_y = (8.9599*plot_x) -35.5762\n",
    "plt.plot(plot_x, plot_y, 'r')\n",
    "plt.xlabel('RM')\n",
    "plt.ylabel('MEDV')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9CI66kKjl7O"
   },
   "source": [
    "We see there's a line the same as the result from 1.1, as with KFold method, we can find the optimized result even with limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cblgmgiBjl7P"
   },
   "source": [
    "## Part 2: Gradient descent: Linear Regression\n",
    "\n",
    "This is where it gets fun!\n",
    "\n",
    "### 2.1 Implement gradient descent with one independent variable (average rooms per house)\n",
    "\n",
    "Implement the batch gradient descent algorithm that we discussed in class. Use the version you implement to regress the housing price on the number of rooms per house. Experiment with 3-4 different values of the learning rate *R*, and do the following:\n",
    "\n",
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of *R*)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "* How do your coefficients compare to the ones estimated through standard libraries? Does this depend on *R*?\n",
    "\n",
    "Some skeleton code is provided below, but you should feel free to delete this code and start from scratch if you prefer.\n",
    "\n",
    "* *Hint 1: Don't forget to implement a stopping condition, so that at every iteration you check whether your results have converged. Common approaches to this are to (a) check to see if the loss has stopped decreasing; and (b) check if both your current parameter esimates are close to the estimates from the previous iteration.  In both cases, \"close\" should not be ==0, it should be <=epsilon, where epsilon is something very small (like 0.0001).*\n",
    "* *Hint 2: Some people like to include a MaxIterations parameter in their gradient descent algorithm, to prevent divergence. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "yAfAQ5EQjl7Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.01\n",
      "Time taken: 0.52 seconds\n",
      "The alpha value is:  -34.71084330598299\n",
      "The beta value is:  8.82801927998741\n",
      "The loss:  22.25031408959191\n",
      "The iterations took is:  34232 \n",
      "\n",
      "Learning rate:  0.001\n",
      "Time taken: 1.55 seconds\n",
      "The alpha value is:  -21.796742796883024\n",
      "The beta value is:  6.859514530068914\n",
      "The loss:  23.355441866019934\n",
      "The iterations took is:  100000 \n",
      "\n",
      "Learning rate:  0.0001\n",
      "Time taken: 1.50 seconds\n",
      "The alpha value is:  2.9403234777936045\n",
      "The beta value is:  3.0888274097048987\n",
      "The loss:  30.914554225415674\n",
      "The iterations took is:  100000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "bivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalues, yvalues : narray\n",
    "    xvalues: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta: float\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "# y = 8.9599*X -35.5762\n",
    "\n",
    "def bivariate_ols(xvalues, yvalues, R=0.01, MaxIterations=1000):\n",
    "    # initialize the parameters\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.000001\n",
    "    alpha = 8\n",
    "    beta = 5\n",
    "    N = len(xvalues)\n",
    "    cost = []\n",
    "    for _ in range(MaxIterations):\n",
    "        f = alpha + beta*xvalues - yvalues\n",
    "        \n",
    "        alpha -= R * (f.sum() / N)\n",
    "        beta -= R * (xvalues.dot(f).sum() /  N)\n",
    "        \n",
    "        cost.append((f**2).sum() / (2*N))\n",
    "    \n",
    "        if len(cost) >=2 and abs(cost[-2] - cost[-1]) <= epsilon:\n",
    "            break\n",
    "        \n",
    "\n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return alpha, beta, cost[-1], len(cost)\n",
    "\n",
    "# example function call\n",
    "\n",
    "X = np.array(boston['RM'])\n",
    "Y = np.array(boston['MEDV'])\n",
    "\n",
    "\n",
    "for rate in [0.01, 0.001, 0.0001]:\n",
    "    print(\"Learning rate: \", rate)\n",
    "    alpha, beta, loss, iterations = bivariate_ols(X, Y, rate, 100000)\n",
    "    print(\"The alpha value is: \", alpha)\n",
    "    print(\"The beta value is: \", beta)\n",
    "    print(\"The loss: \", loss)\n",
    "    print(\"The iterations took is: \", iterations, \"\\n\")\n",
    "\n",
    "\n",
    "# print(\"Learning rate: 0.01\")\n",
    "# print(bivariate_ols(X, Y, 0.01, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.001\")\n",
    "# print(bivariate_ols(X, Y, 0.001, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.0001\")\n",
    "# print(bivariate_ols(X, Y, 0.0001, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7dK9bMZjl7Q"
   },
   "source": [
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of R)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Shown as above results.\n",
    "\n",
    "* How do your coefficients compare to the ones estimated through standard libraries? Does this depend on R?\n",
    "\n",
    "The coefficient in 1.1 is (alpha, beta) = (-35.5762, 8.9599)\n",
    "The results from learning rate at 0.01 is the closest to the results from the standard library. This is because when the learning rates are too small (0.001 & 0.0001), so we can't find the optimized (alpha, beta) within the iteration limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZqg5yW3jl7Q"
   },
   "source": [
    "### 2.2 Data normalization (done for you!)\n",
    "\n",
    "Soon, you will implement a version of gradient descent that can use an arbitrary number of independent variables. Before doing this, we want to give you some code in case you want to standardize your features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lU9-kG3Tjl7Q"
   },
   "outputs": [],
   "source": [
    "def standardize(raw_data):\n",
    "    return ((raw_data - np.mean(raw_data, axis = 0)) / np.std(raw_data, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUUackd0jl7R"
   },
   "source": [
    "### 2.3 Implement gradient descent with an arbitrary number of independent variables\n",
    "\n",
    "Now that you have a simple version of gradient descent working, create a version of gradient descent that can take more than one independent variable.  Assume all independent variables will be continuous.  Test your algorithm using TAX and RM as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. \n",
    "\n",
    "As before,  report and interpret your estimated coefficients, the number of iterations before convergence, and the total running time of your algorithm. Experiment with 2-3 different values of R.\n",
    "\n",
    "* *Hint 1: Be careful to implement this efficiently, otherwise it might take a long time for your code to run. Commands like `np.dot` can be a good friend to you on this problem*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "id": "yhEcthNtjl7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.01\n",
      "Time taken: 0.29 seconds\n",
      "The alpha value is:  [22.5322371]\n",
      "The beta_array is:  [[ 5.53823667]\n",
      " [-2.72185118]]\n",
      "The loss:  24.438563857934398\n",
      "The iterations took is:  523 \n",
      "\n",
      "Learning rate:  0.001\n",
      "Time taken: 2.62 seconds\n",
      "The alpha value is:  [22.53132888]\n",
      "The beta_array is:  [[ 5.5377043 ]\n",
      " [-2.72236431]]\n",
      "The loss:  24.43874501259081\n",
      "The iterations took is:  4799 \n",
      "\n",
      "Learning rate:  0.0001\n",
      "Time taken: 23.05 seconds\n",
      "The alpha value is:  [22.5240198]\n",
      "The beta_array is:  [[ 5.53458197]\n",
      " [-2.72518652]]\n",
      "The loss:  24.441948655708664\n",
      "The iterations took is:  38670 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "multivariate_ols\n",
    "    Gradient Decent to minimize OLS. Used to find co-efficients of bivariate OLS Linear regression\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "xvalue_matrix, yvalues : narray\n",
    "    xvalue_matrix: independent variable\n",
    "    yvalues: dependent variable\n",
    "    \n",
    "R: float\n",
    "    Learning rate\n",
    "    \n",
    "MaxIterations: Int\n",
    "    maximum number of iterations\n",
    "    \n",
    "\n",
    "Returns\n",
    "-------\n",
    "alpha: float\n",
    "    intercept\n",
    "    \n",
    "beta_array: array[float]\n",
    "    co-efficient\n",
    "\"\"\"\n",
    "\n",
    "def createXvector(xvalue_matrix):\n",
    "    vectorX = np.c_[np.ones((len(xvalue_matrix), 1)), xvalue_matrix]\n",
    "    return vectorX\n",
    "\n",
    "def createTheta(xvalue_matrix):\n",
    "    theta = np.random.randn(len(xvalue_matrix[0])+1, 1)\n",
    "    return theta\n",
    "\n",
    "def multivariate_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000):\n",
    "    # your code here\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.000001\n",
    "    \n",
    "    y_reshaped = np.reshape(yvalues, (len(yvalues), 1))\n",
    "    costs = []\n",
    "    vectorX = createXvector(xvalue_matrix)\n",
    "    theta = createTheta(xvalue_matrix)\n",
    "    m = len(xvalue_matrix)\n",
    "    \n",
    "    for _ in range(MaxIterations):\n",
    "        gradients = 2/m * vectorX.T.dot(vectorX.dot(theta) - y_reshaped)\n",
    "        theta = theta - R * gradients\n",
    "        y_pred = vectorX.dot(theta)\n",
    "        cost = 1/(2*len(yvalues))*((y_pred - yvalues)**2)\n",
    "#         print(cost)\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(yvalues)):\n",
    "            total += cost[i][0]\n",
    "        costs.append(total)\n",
    "#         print(total)\n",
    "        \n",
    "        if len(costs) >=2 and abs(costs[-2] - costs[-1]) <= epsilon:\n",
    "                break  \n",
    "        \n",
    "    \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return theta[0], theta[1:], costs[-1], len(costs)\n",
    "#     return alpha, beta_array\n",
    "\n",
    "X_matrix = np.column_stack((np.array(boston['RM']), np.array(boston['TAX'])))\n",
    "Y = np.array(boston['MEDV'])\n",
    "\n",
    "\n",
    "for rate in [0.01, 0.001, 0.0001]:\n",
    "    print(\"Learning rate: \", rate)\n",
    "    alpha, beta_array, loss, iterations = multivariate_ols(standardize(X_matrix), Y, rate, 100000)\n",
    "    print(\"The alpha value is: \", alpha)\n",
    "    print(\"The beta_array is: \", beta_array)\n",
    "    print(\"The loss: \", loss)\n",
    "    print(\"The iterations took is: \", iterations, \"\\n\")\n",
    "    \n",
    "# alpha, beta_array, loss, iterations = multivariate_ols(standardize(X_matrix), Y, 0.01, 100000)\n",
    "# print(multivariate_ols(standardize(X_matrix), Y, 0.01, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.001\")\n",
    "# print(multivariate_ols(standardize(X_matrix), Y, 0.001, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.0001\")\n",
    "# print(multivariate_ols(standardize(X_matrix), Y, 0.0001, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2o_SDdcjl7R"
   },
   "source": [
    "* Report the values of alpha and beta that minimize the loss function\n",
    "* Report the number of iterations it takes for your algorithm to converge (for each value of R)\n",
    "* Report the total running time of your algorithm, in seconds\n",
    "\n",
    "Shown as above results.\n",
    "\n",
    "* Interpretation\n",
    "\n",
    "The results from the three learning rates are quite similar, as we are able to find the minimun loss within iteration limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMXw_qcYjl7S"
   },
   "source": [
    "### 2.4 Compare standardized vs. non-standardized results\n",
    "\n",
    "Repeat the analysis from 2.3, but this time do not standardize your variables - i.e., use the original data. Use the same three values of R (0.1, 0.01, and 0.001). What do you notice about the running time and convergence properties of your algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "AnEvJxxhjl7S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z7/qfh431gd2hl9fztg2f0y4h7r0000gn/T/ipykernel_3008/1776497120.py:52: RuntimeWarning: overflow encountered in square\n",
      "  cost = 1/(2*len(yvalues))*((y_pred - yvalues)**2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 53.75 seconds\n",
      "The alpha value is:  [nan]\n",
      "The beta_array is:  [[nan]\n",
      " [nan]]\n",
      "The loss:  nan\n",
      "The iterations took is:  100000 \n",
      "\n",
      "Learning rate:  0.01\n",
      "Time taken: 56.61 seconds\n",
      "The alpha value is:  [nan]\n",
      "The beta_array is:  [[nan]\n",
      " [nan]]\n",
      "The loss:  nan\n",
      "The iterations took is:  100000 \n",
      "\n",
      "Learning rate:  0.001\n",
      "Time taken: 57.80 seconds\n",
      "The alpha value is:  [nan]\n",
      "The beta_array is:  [[nan]\n",
      " [nan]]\n",
      "The loss:  nan\n",
      "The iterations took is:  100000 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "for rate in [0.1, 0.01, 0.001]:\n",
    "    print(\"Learning rate: \", rate)\n",
    "    alpha, beta_array, loss, iterations = multivariate_ols(X_matrix, Y, rate, 100000)\n",
    "    print(\"The alpha value is: \", alpha)\n",
    "    print(\"The beta_array is: \", beta_array)\n",
    "    print(\"The loss: \", loss)\n",
    "    print(\"The iterations took is: \", iterations, \"\\n\")\n",
    "\n",
    "# print(\"Learning rate: 0.1\")\n",
    "# print(multivariate_ols(X_matrix, Y, 0.1, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.01\")\n",
    "# print(multivariate_ols(X_matrix, Y, 0.01, 100000))\n",
    "\n",
    "# print(\"Learning rate: 0.001\")\n",
    "# print(multivariate_ols(X_matrix, Y, 0.001, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFdApjEsjl7S"
   },
   "source": [
    "If we don't standardize our variables, the function will not be able to converge, and the cost will be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pEk2_UZjl7S"
   },
   "source": [
    "## 3. Prediction\n",
    "\n",
    "Let's use our fitted model to make predictions about housing prices. Make sure to first standardize your features before proceeding.\n",
    "\n",
    "### 3.1 Cross-Validation\n",
    "\n",
    "Unless you were careful above, you probably overfit your data again. Let's fix that. Use 5-fold cross-validation to re-fit the multivariate regression from 2.3 above, and report your estimated coefficients (there should be three, corresponding to the intercept and the two coefficients for TAX and RM). Since there are 5 folds, there will be 5 sets of three coefficients -- report them all in a 5x3 table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.8.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "id": "DGQRERuIjl7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.28 seconds\n",
      "Time taken: 0.24 seconds\n",
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.21 seconds\n",
      "Time taken: 0.19 seconds\n",
      "  Intercept    RM coefficient    TAX coefficient\n",
      "-----------  ----------------  -----------------\n",
      "    23.044            5.25849           -3.14676\n",
      "    22.2681           4.90349           -2.73388\n",
      "    21.937            4.54228           -2.4925\n",
      "    22.22             6.65563           -3.26529\n",
      "    23.1918           6.15649           -1.65043\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from tabulate import tabulate\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "X_matrix_standardized = standardize(X_matrix)\n",
    "\n",
    "values = []\n",
    "for train_index, test_index in kf.split(X_matrix_standardized):\n",
    "    alpha, beta_arr, _, _ = multivariate_ols(X_matrix_standardized[train_index], Y[train_index])\n",
    "    \n",
    "    value = [alpha]\n",
    "    value.append(beta_arr[0])\n",
    "    value.append(beta_arr[1])\n",
    "    values.append(value)\n",
    "\n",
    "head = [\"Intercept\", \"RM coefficient\", \"TAX coefficient\"]\n",
    "print(tabulate(values, headers=head))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHk88-z5jl7T"
   },
   "source": [
    "The result from 2.3\n",
    "\n",
    "Learning rate:  0.01\n",
    "Time taken: 0.29 seconds\n",
    "The alpha value is:  [22.5322371]\n",
    "The beta_array is:  [[ 5.53823667]\n",
    " [-2.72185118]]\n",
    "The loss:  24.438563857934398\n",
    "The iterations took is:  523 \n",
    "\n",
    "Learning rate:  0.001\n",
    "Time taken: 2.62 seconds\n",
    "The alpha value is:  [22.53132888]\n",
    "The beta_array is:  [[ 5.5377043 ]\n",
    " [-2.72236431]]\n",
    "The loss:  24.43874501259081\n",
    "The iterations took is:  4799 \n",
    "\n",
    "Learning rate:  0.0001\n",
    "Time taken: 23.05 seconds\n",
    "The alpha value is:  [22.5240198]\n",
    "The beta_array is:  [[ 5.53458197]\n",
    " [-2.72518652]]\n",
    "The loss:  24.441948655708664\n",
    "The iterations took is:  38670 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from KFold vary a bit, as a result, we shoe exam what's the best model and if there's overfitting problem from previous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaRVr5wpjl7T"
   },
   "source": [
    "### 3.2 Predicted values and RMSE\n",
    "\n",
    "Let's figure out how accurate this predictive model turned out to be. Compute the cross-validated RMSE for each of the 5 folds above. In other words, in fold 1, use the parameters estimated on the 80% of the data to make predictions for the 20%, and calculate the RMSE for those 20%. Repeate this for the remaining folds. Report the RMSE for each of the 5-folds, and the average (mean) RMSE across the five folds. How does this average RMSE compare to the performance of your nearest neighbor algorithm from the last problem set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: compute_rmse function taken from PS3\n",
    "def compute_rmse(predictions, yvalues):\n",
    "    pre = np.asarray(predictions)\n",
    "    y = np.asarray(yvalues)\n",
    "    rmse = np.sqrt(np.sum((pre-y) ** 2) / float(len(y)))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "id": "9I1zFLBajl7T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.23 seconds\n",
      "Time taken: 0.29 seconds\n",
      "Time taken: 0.23 seconds\n",
      "Time taken: 0.18 seconds\n",
      "[4.253799623999337, 5.567110862985532, 5.537024727633761, 10.587117087372004, 6.773439076923077]\n",
      "the average RMSE is 6.543698275782742\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# kf = KFold(n_splits=5)\n",
    "# X_matrix_standardized = standardize(X_matrix)\n",
    "\n",
    "RMSE = []\n",
    "for train_index, test_index in kf.split(X_matrix_standardized):\n",
    "    alpha, beta_arr, _, _ = multivariate_ols(X_matrix_standardized[train_index], Y[train_index])\n",
    "    theta = []\n",
    "    theta.append(alpha[0])\n",
    "    for i in range(len(beta_arr)):\n",
    "        theta.append(beta_arr[i][0])\n",
    "\n",
    "#     y_hat = [] \n",
    "#     for i in range(len(test_index)):\n",
    "#         value = alpha + sum(beta_arr[0]*X_matrix_standardized[test_index][i][0], beta_arr[1]*X_matrix_standardized[test_index][i][1])\n",
    "#         y_hat.append(value)\n",
    "\n",
    "    X_mat = np.c_[np.ones((len(test_index), 1)), X_matrix_standardized[test_index]]\n",
    "    y_hat = X_mat.dot(theta)\n",
    "    RMSE.append(compute_rmse(y_hat, Y[test_index]))\n",
    "\n",
    "print(RMSE)\n",
    "print(\"the average RMSE is\", np.mean(RMSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61FuGxiYjl7T"
   },
   "source": [
    "The RMSE I got from PS3 with the same independent variables is 9.81. We see the RMSE is much smaller here, as KFold method can help us find the optimized result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L-AsWOajl7T"
   },
   "source": [
    "### Extra Credit 1: Logistic Regression\n",
    "\n",
    "For extra credit, implement logistic regression using gradient descent. Create a new variable (EXPENSIVE) to indicate whether the median housing price is more than $40,000. Use your model  a logistic regression of EXPENSIVE on CHAS and RM. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UR9qYdndjl7T"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04AamlXLjl7T"
   },
   "source": [
    "*Discuss your results here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvkpICWsjl7T"
   },
   "source": [
    "## 4 Regularization \n",
    "\n",
    "### 4.1 Get prepped\n",
    "\n",
    "Step 1: Create new interaction variables between each possible pair of the F_s features. If you originally had *K* features, you should now have K+(K*(K+1))/2 features. Standardize all of your features.\n",
    "\n",
    "Step 2: Randomly sample 80% of your data and call this the training set, and set aside the remaining 20% as your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "89-Zqpssjl7U"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CRIM'],\n",
       " ['CRIM', 'CRIM'],\n",
       " ['ZN'],\n",
       " ['ZN', 'CRIM'],\n",
       " ['ZN', 'ZN'],\n",
       " ['INDUS'],\n",
       " ['INDUS', 'CRIM'],\n",
       " ['INDUS', 'ZN'],\n",
       " ['INDUS', 'INDUS'],\n",
       " ['CHAS'],\n",
       " ['CHAS', 'CRIM'],\n",
       " ['CHAS', 'ZN'],\n",
       " ['CHAS', 'INDUS'],\n",
       " ['CHAS', 'CHAS'],\n",
       " ['NOX'],\n",
       " ['NOX', 'CRIM'],\n",
       " ['NOX', 'ZN'],\n",
       " ['NOX', 'INDUS'],\n",
       " ['NOX', 'CHAS'],\n",
       " ['NOX', 'NOX'],\n",
       " ['RM'],\n",
       " ['RM', 'CRIM'],\n",
       " ['RM', 'ZN'],\n",
       " ['RM', 'INDUS'],\n",
       " ['RM', 'CHAS'],\n",
       " ['RM', 'NOX'],\n",
       " ['RM', 'RM'],\n",
       " ['AGE'],\n",
       " ['AGE', 'CRIM'],\n",
       " ['AGE', 'ZN'],\n",
       " ['AGE', 'INDUS'],\n",
       " ['AGE', 'CHAS'],\n",
       " ['AGE', 'NOX'],\n",
       " ['AGE', 'RM'],\n",
       " ['AGE', 'AGE'],\n",
       " ['DIS'],\n",
       " ['DIS', 'CRIM'],\n",
       " ['DIS', 'ZN'],\n",
       " ['DIS', 'INDUS'],\n",
       " ['DIS', 'CHAS'],\n",
       " ['DIS', 'NOX'],\n",
       " ['DIS', 'RM'],\n",
       " ['DIS', 'AGE'],\n",
       " ['DIS', 'DIS'],\n",
       " ['RAD'],\n",
       " ['RAD', 'CRIM'],\n",
       " ['RAD', 'ZN'],\n",
       " ['RAD', 'INDUS'],\n",
       " ['RAD', 'CHAS'],\n",
       " ['RAD', 'NOX'],\n",
       " ['RAD', 'RM'],\n",
       " ['RAD', 'AGE'],\n",
       " ['RAD', 'DIS'],\n",
       " ['RAD', 'RAD'],\n",
       " ['TAX'],\n",
       " ['TAX', 'CRIM'],\n",
       " ['TAX', 'ZN'],\n",
       " ['TAX', 'INDUS'],\n",
       " ['TAX', 'CHAS'],\n",
       " ['TAX', 'NOX'],\n",
       " ['TAX', 'RM'],\n",
       " ['TAX', 'AGE'],\n",
       " ['TAX', 'DIS'],\n",
       " ['TAX', 'RAD'],\n",
       " ['TAX', 'TAX'],\n",
       " ['PTRATIO'],\n",
       " ['PTRATIO', 'CRIM'],\n",
       " ['PTRATIO', 'ZN'],\n",
       " ['PTRATIO', 'INDUS'],\n",
       " ['PTRATIO', 'CHAS'],\n",
       " ['PTRATIO', 'NOX'],\n",
       " ['PTRATIO', 'RM'],\n",
       " ['PTRATIO', 'AGE'],\n",
       " ['PTRATIO', 'DIS'],\n",
       " ['PTRATIO', 'RAD'],\n",
       " ['PTRATIO', 'TAX'],\n",
       " ['PTRATIO', 'PTRATIO'],\n",
       " ['B'],\n",
       " ['B', 'CRIM'],\n",
       " ['B', 'ZN'],\n",
       " ['B', 'INDUS'],\n",
       " ['B', 'CHAS'],\n",
       " ['B', 'NOX'],\n",
       " ['B', 'RM'],\n",
       " ['B', 'AGE'],\n",
       " ['B', 'DIS'],\n",
       " ['B', 'RAD'],\n",
       " ['B', 'TAX'],\n",
       " ['B', 'PTRATIO'],\n",
       " ['B', 'B'],\n",
       " ['LSTAT'],\n",
       " ['LSTAT', 'CRIM'],\n",
       " ['LSTAT', 'ZN'],\n",
       " ['LSTAT', 'INDUS'],\n",
       " ['LSTAT', 'CHAS'],\n",
       " ['LSTAT', 'NOX'],\n",
       " ['LSTAT', 'RM'],\n",
       " ['LSTAT', 'AGE'],\n",
       " ['LSTAT', 'DIS'],\n",
       " ['LSTAT', 'RAD'],\n",
       " ['LSTAT', 'TAX'],\n",
       " ['LSTAT', 'PTRATIO'],\n",
       " ['LSTAT', 'B'],\n",
       " ['LSTAT', 'LSTAT']]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "# step 1-1: create combinations as F_s_features\n",
    "features = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"]\n",
    "\n",
    "N = len(features)\n",
    "F_s_features = []\n",
    "for i in range(N):\n",
    "    F_s_features.append([features[i]])\n",
    "    for j in range(i+1):\n",
    "        F_s_features.append([features[i], features[j]])\n",
    "\n",
    "\n",
    "len(F_s_features)\n",
    "F_s_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1-2: add variable-squared data in boston dataset\n",
    "for feature in features:\n",
    "    boston[feature+'2'] = boston[feature]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: shuffle the data \n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "ids = np.arange(0, len(boston), 1)\n",
    "ids = np.random.permutation(ids)\n",
    "boston_shuffled = boston.iloc[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "506\n"
     ]
    }
   ],
   "source": [
    "# step 1-3: prepare data in matrix and standardize them as F_s_standardized\n",
    "\n",
    "F_s_standardized = []\n",
    "\n",
    "for pair in F_s_features:\n",
    "    if len(pair) == 1:\n",
    "        curr_matrix = np.column_stack(np.array(boston_shuffled[pair]))\n",
    "        F_s_standardized.append(standardize(curr_matrix[0]))\n",
    "    elif pair[0] == pair[1]:\n",
    "        curr_matrix = np.column_stack(np.array(boston_shuffled[pair[0]+'2']))\n",
    "        F_s_standardized.append(standardize(curr_matrix[0]))\n",
    "    else:\n",
    "        curr_matrix = np.column_stack((np.array(boston_shuffled[pair[0]]), np.array(boston_shuffled[pair[1]])))\n",
    "        F_s_standardized.append(standardize(curr_matrix))\n",
    "\n",
    "\n",
    "print(len(F_s_standardized))\n",
    "print(len(F_s_standardized[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n",
      "404\n",
      "104\n",
      "102\n",
      "404\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "# step 2-2: seperate into test and training datasets\n",
    "# credit: lab4 solution\n",
    "\n",
    "train_percent = .80\n",
    "train_number = int(train_percent*len(boston))\n",
    "# print('Total examples: %i' % len(boston))\n",
    "# print('Number of training examples: %i' % train_number)\n",
    "# print('Number of testing examples: %i' % (len(boston) - train_number))\n",
    "\n",
    "# np.random.seed(1)\n",
    "\n",
    "# ids = np.arange(0, len(boston), 1)\n",
    "# ids = np.random.permutation(ids)\n",
    "# boston_shuffled = boston.iloc[ids]\n",
    "\n",
    "y_train_set = boston_shuffled['MEDV'][:train_number]\n",
    "y_test_set = boston_shuffled['MEDV'][train_number:]\n",
    "\n",
    "X_train_sets = []\n",
    "X_test_sets = []\n",
    "\n",
    "for dataset in F_s_standardized:\n",
    "    X_train_sets.append(dataset[:train_number])\n",
    "    X_test_sets.append(dataset[train_number:])\n",
    "    \n",
    "print(len(X_train_sets))\n",
    "print(len(X_train_sets[3]))\n",
    "print(len(X_test_sets))\n",
    "print(len(X_test_sets[3]))\n",
    "print(len(y_train_set))\n",
    "print(len(y_test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeXkvp7njl7U"
   },
   "source": [
    "### 4.2 Overfitting (sort of)\n",
    "Now, using your version of multivariate regression from 2.3, let's overfit the training data. Using your training set, regress housing price on as many of those K+(K*(K+1))/2 features as you can (Don't forget to add quadratic terms. Form instance, RM^2.).  If you get too greedy, it's possible this will take a long time to compute, so start with 5-10 features, and if you have the time, add more features.\n",
    "\n",
    "Report the RMSE when you apply your model to your training set and to your testing set. How do these numbers compare to each other, and to the RMSE from 3.2 and nearest neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "id": "kgQ48hi6jl7U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features set is ['RAD', 'RM']\n",
      "Time taken: 0.06 seconds\n",
      "The RMSE for test set: 6.230594501814128\n",
      "The RMSE for train set: 6.302344529230654\n",
      "The features set is ['RAD', 'AGE']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 8.527262005290995\n",
      "The RMSE for train set: 8.157333599349874\n",
      "The features set is ['RAD', 'DIS']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 8.854725790138966\n",
      "The RMSE for train set: 8.375274945999099\n",
      "The features set is ['RAD', 'RAD']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 8.878634676870426\n",
      "The RMSE for train set: 8.350195403488142\n",
      "The features set is ['TAX']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 8.366198729927625\n",
      "The RMSE for train set: 8.051549353647717\n",
      "The features set is ['TAX', 'CRIM']\n",
      "Time taken: 0.06 seconds\n",
      "The RMSE for test set: 8.23972618716439\n",
      "The RMSE for train set: 7.95536796976632\n",
      "The features set is ['TAX', 'ZN']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 7.932054943276837\n",
      "The RMSE for train set: 7.832741856815916\n",
      "The features set is ['TAX', 'INDUS']\n",
      "Time taken: 0.08 seconds\n",
      "The RMSE for test set: 7.967502516827409\n",
      "The RMSE for train set: 7.864644534782901\n",
      "The features set is ['TAX', 'CHAS']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 8.3627664117381\n",
      "The RMSE for train set: 7.888132892601139\n",
      "The features set is ['TAX', 'NOX']\n",
      "Time taken: 0.06 seconds\n",
      "The RMSE for test set: 8.257091151290707\n",
      "The RMSE for train set: 8.038913003027707\n",
      "The features set is ['TAX', 'RM']\n",
      "Time taken: 0.05 seconds\n",
      "The RMSE for test set: 6.048491814641932\n",
      "The RMSE for train set: 6.164080381844094\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "for i in range(50,61):\n",
    "    print(\"The features set is\", F_s_features[i])\n",
    "    \n",
    "    coefs = []\n",
    "    if len(X_train_sets[i].shape)== 2 and X_train_sets[i].shape[1] == 2:\n",
    "        alpha, beta_arr, _, _ = multivariate_ols(X_train_sets[i], np.array(y_train_set).reshape(-1,1))\n",
    "        coefs.append(alpha[0])\n",
    "        for j in range(len(beta_arr)):\n",
    "            coefs.append(beta_arr[j][0])\n",
    "    else:\n",
    "        alpha, beta_arr, _, _ = multivariate_ols(np.array(X_train_sets[i]).reshape(-1,1), np.array(y_train_set).reshape(-1,1))\n",
    "        coefs.append(alpha[0])\n",
    "        for j in range(len(beta_arr)):\n",
    "            coefs.append(beta_arr[j][0])        \n",
    "#     coefs = []\n",
    "#     coefs.append(alpha[0])\n",
    "#     for j in range(len(beta_arr)):\n",
    "#         coefs.append(beta_arr[j][0])\n",
    "    \n",
    "#     print(coefs)\n",
    "    X_mat_test = np.c_[np.ones((len(X_test_sets[i]), 1)), X_test_sets[i]]\n",
    "    y_hat_test = X_mat_test.dot(coefs)\n",
    "    print(\"The RMSE for test set:\", compute_rmse(y_hat_test, y_test_set))\n",
    "\n",
    "    X_mat_train = np.c_[np.ones((len(X_train_sets[i]), 1)), X_train_sets[i]]\n",
    "    y_hat_train = X_mat_train.dot(coefs)\n",
    "    print(\"The RMSE for train set:\", compute_rmse(y_hat_train, y_train_set))\n",
    "\n",
    "\n",
    "#     if len(X_train_sets[i].shape)== 2 and X_train_sets[i].shape[1] == 2:\n",
    "#         alpha, beta_arr, _, _ = multivariate_ols(X_train_sets[i], y_train_set)\n",
    "        \n",
    "#         y_hat = lm.predict(np.array(X_test_sets[i]))\n",
    "#         print(\"The RMSE for test set\", compute_rmse(y_hat, y_test_set))\n",
    "\n",
    "#         y_hat = lm.predict(np.array(X_train_sets[i]))\n",
    "#         print(\"The RMSE for train set\", compute_rmse(y_hat, y_train_set))\n",
    "        \n",
    "#     else:\n",
    "#         lm = LinearRegression().fit(np.array(X_train_sets[i]).reshape(-1,1), y_train_set)\n",
    "    \n",
    "#         y_hat = lm.predict(np.array(X_test_sets[i]).reshape(-1,1))\n",
    "#         print(\"The RMSE for test set\", compute_rmse(y_hat, y_test_set))\n",
    "\n",
    "#         y_hat = lm.predict(np.array(X_train_sets[i]).reshape(-1,1))\n",
    "#         print(\"The RMSE for train set\", compute_rmse(y_hat, y_train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wf3h2JEIjl7U"
   },
   "source": [
    "The RMSE between test and train sets are pretty similar, which means the model is underfitting or well-fit. Compared to the result from 2.3 with the average RMSE is 6.543698275782742, the RMSE here are all higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDlDsi4wjl7U"
   },
   "source": [
    "### 4.3 Ridge regularization (basic)\n",
    "Incorporate L2 (Ridge) regularization into your multivariate_ols regression. Write a new version of your gradient descent algorithm that includes a regularization term \"lambda\" to penalize excessive complexity. \n",
    "\n",
    "Use your regularized regression to re-fit the model from 4.2 above on your training data, using the value lambda = 0.5.  Report the RMSE obtained for your training data, and the RMSE obtained for your testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "Rlaok4imjl7U"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def ridge_regularization_multivariate_ols(xvalue_matrix, yvalues, R=0.01, MaxIterations=1000, L=0.5):\n",
    "    # your code here\n",
    "    start_time = time.time()\n",
    "    epsilon = 0.000001\n",
    "    \n",
    "    y_reshaped = np.reshape(yvalues, (len(yvalues), 1))\n",
    "    costs = []\n",
    "    vectorX = createXvector(xvalue_matrix)\n",
    "    theta = createTheta(xvalue_matrix)\n",
    "    m = len(xvalue_matrix)\n",
    "    \n",
    "    for _ in range(MaxIterations):\n",
    "        gradients = 2/m * vectorX.T.dot(vectorX.dot(theta) - y_reshaped)\n",
    "        theta = theta - (theta*R*L)/m - R * gradients\n",
    "        y_pred = vectorX.dot(theta)\n",
    "        ridge_reg_term = ((L/(2*len(yvalues)))*(np.sum(np.square(theta))))\n",
    "        cost = (1/(2*len(yvalues))*((y_pred - yvalues)**2)) + ridge_reg_term\n",
    "#         print(cost)\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(yvalues)):\n",
    "            total += cost[i][0]\n",
    "        costs.append(total)\n",
    "#         print(total)\n",
    "        \n",
    "        if len(costs) >=2 and abs(costs[-2] - costs[-1]) <= epsilon:\n",
    "                break  \n",
    "        \n",
    "    \n",
    "    print(\"Time taken: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return theta[0], theta[1:], costs[-1], len(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features set is ['RAD', 'RM']\n",
      "Time taken: 0.12 seconds\n",
      "The RMSE for test set: 6.231486989848541\n",
      "The RMSE for train set: 6.3023567826547895\n",
      "The features set is ['RAD', 'AGE']\n",
      "Time taken: 0.10 seconds\n",
      "The RMSE for test set: 8.526975873918818\n",
      "The RMSE for train set: 8.157342723716503\n",
      "The features set is ['RAD', 'DIS']\n",
      "Time taken: 0.12 seconds\n",
      "The RMSE for test set: 8.855181210680303\n",
      "The RMSE for train set: 8.375281706250954\n",
      "The features set is ['RAD', 'RAD']\n",
      "Time taken: 0.10 seconds\n",
      "The RMSE for test set: 8.877997523281275\n",
      "The RMSE for train set: 8.350204639880697\n",
      "The features set is ['TAX']\n",
      "Time taken: 0.10 seconds\n",
      "The RMSE for test set: 8.36560072739786\n",
      "The RMSE for train set: 8.051559070893152\n",
      "The features set is ['TAX', 'CRIM']\n",
      "Time taken: 0.13 seconds\n",
      "The RMSE for test set: 8.23891445607453\n",
      "The RMSE for train set: 7.955373276105389\n",
      "The features set is ['TAX', 'ZN']\n",
      "Time taken: 0.11 seconds\n",
      "The RMSE for test set: 7.9321537943241855\n",
      "The RMSE for train set: 7.832750807106321\n",
      "The features set is ['TAX', 'INDUS']\n",
      "Time taken: 0.13 seconds\n",
      "The RMSE for test set: 7.968217554497515\n",
      "The RMSE for train set: 7.864645770882019\n",
      "The features set is ['TAX', 'CHAS']\n",
      "Time taken: 0.12 seconds\n",
      "The RMSE for test set: 8.362099312224847\n",
      "The RMSE for train set: 7.888142862468585\n",
      "The features set is ['TAX', 'NOX']\n",
      "Time taken: 0.19 seconds\n",
      "The RMSE for test set: 8.25763127784495\n",
      "The RMSE for train set: 8.038918966808412\n",
      "The features set is ['TAX', 'RM']\n",
      "Time taken: 0.11 seconds\n",
      "The RMSE for test set: 6.049216627898531\n",
      "The RMSE for train set: 6.164092742585029\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 61):\n",
    "    print(\"The features set is\", F_s_features[i])\n",
    "    coefs = []\n",
    "    if len(X_train_sets[i].shape)== 2 and X_train_sets[i].shape[1] == 2:\n",
    "        alpha, beta_arr, _, _ = ridge_regularization_multivariate_ols(X_train_sets[i], np.array(y_train_set).reshape(-1,1))\n",
    "        coefs.append(alpha[0])\n",
    "        for j in range(len(beta_arr)):\n",
    "            coefs.append(beta_arr[j][0])\n",
    "    else:\n",
    "        alpha, beta_arr, _, _ = ridge_regularization_multivariate_ols(np.array(X_train_sets[i]).reshape(-1,1), np.array(y_train_set).reshape(-1,1))\n",
    "        coefs.append(alpha[0])\n",
    "        for j in range(len(beta_arr)):\n",
    "            coefs.append(beta_arr[j][0])\n",
    "#     print(coefs)\n",
    "#     coefs = []\n",
    "#     coefs.append(alpha[0])\n",
    "#     for j in range(len(beta_arr)):\n",
    "#         coefs.append(beta_arr[j][0])\n",
    "    \n",
    "    X_mat_test = np.c_[np.ones((len(X_test_sets[i]), 1)), X_test_sets[i]]\n",
    "    y_hat_test = X_mat_test.dot(coefs)\n",
    "    print(\"The RMSE for test set:\", compute_rmse(y_hat_test, y_test_set))\n",
    "    \n",
    "    X_mat_train = np.c_[np.ones((len(X_train_sets[i]), 1)), X_train_sets[i]]\n",
    "    y_hat_train = X_mat_train.dot(coefs)\n",
    "    print(\"The RMSE for train set:\", compute_rmse(y_hat_train, y_train_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxZdfCKvjl7U"
   },
   "source": [
    "Compared to the results in 4.2, the gap between the RMSE for test set and the RMSE for train set become a bit closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIYjxCfAjl7U"
   },
   "source": [
    "### 4.4: Cross-validate lambda\n",
    "\n",
    "This is where it all comes together! Use k-fold cross-validation to select the optimal value of lambda. In other words, define a set of different values of lambda. Then, using the 80% of your data that you set aside for training, iterate through the values of lambda one at a time. For each value of lambda, use k-fold cross-validation to compute the average cross-validated (test) RMSE for that lambda value, computed as the average across the held-out folds. You should also record the average cross-validated train RMSE, computed as the average across the folds used for training. Create a scatter plot that shows RMSE as a function of lambda. The scatter plot should have two lines: a red line showing the cross-validated (test) RMSE, and a blue line showing the cross-validated train RMSE.  At this point, you should not have touched your held-out 20% of \"true\" test data.\n",
    "\n",
    "What value of lambda minimizes your cross-validated (test) RMSE? Fix that value of lambda, and train a new model using all of your training data with that value of lambda (i.e., use the entire 80% of the data that you set aside in 4.1). Calcuate the RMSE for this model on the 20% of \"true\" test data. How does your test RMSE compare to the RMSE from 4.3, 4.2, 2.3, and to the RMSE from nearest neighbors? What do you make of these results?\n",
    "\n",
    "Go brag to your friends about how you just implemented cross-validated ridge-regularized multivariate regression using gradient descent optimization, from scratch. If you still have friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0,10,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_data = np.array(y_train_set)\n",
    "y_test_data = np.array(y_test_set)\n",
    "X_train_data = X_train_sets[60]\n",
    "X_test_data = X_test_sets[60]\n",
    "\n",
    "# X_train_sets = []\n",
    "# X_test_sets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "id": "35QDSkeGjl7U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.19 seconds\n",
      "The RMSE for test set: 6.362578647200465\n",
      "The RMSE for train set: 6.123744165379022\n",
      "Time taken: 0.17 seconds\n",
      "The RMSE for test set: 5.805975807019213\n",
      "The RMSE for train set: 6.274761466773228\n",
      "Time taken: 0.21 seconds\n",
      "The RMSE for test set: 6.529641767836626\n",
      "The RMSE for train set: 6.0730583764059896\n",
      "Time taken: 0.21 seconds\n",
      "The RMSE for test set: 5.091023561928174\n",
      "The RMSE for train set: 6.4091125035247565\n",
      "Time taken: 0.22 seconds\n",
      "The RMSE for test set: 7.206619942333616\n",
      "The RMSE for train set: 5.887247637480764\n",
      "Time taken: 0.27 seconds\n",
      "The RMSE for test set: 6.366339822934655\n",
      "The RMSE for train set: 6.123846032406392\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 5.800024018857976\n",
      "The RMSE for train set: 6.2748668209203045\n",
      "Time taken: 0.22 seconds\n",
      "The RMSE for test set: 6.530616285971684\n",
      "The RMSE for train set: 6.07316169267528\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 5.086303285774549\n",
      "The RMSE for train set: 6.409211462581828\n",
      "Time taken: 0.22 seconds\n",
      "The RMSE for test set: 7.209763284973583\n",
      "The RMSE for train set: 5.887352563509298\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 6.370267177506635\n",
      "The RMSE for train set: 6.124150361636326\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 5.794283686856512\n",
      "The RMSE for train set: 6.275181549328584\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 6.531779767497325\n",
      "The RMSE for train set: 6.073470287156121\n",
      "Time taken: 0.22 seconds\n",
      "The RMSE for test set: 5.081831756412664\n",
      "The RMSE for train set: 6.409507048478999\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 7.21306008514261\n",
      "The RMSE for train set: 5.887665975264549\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 6.374367887955597\n",
      "The RMSE for train set: 6.124655300110236\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 5.788819131774554\n",
      "The RMSE for train set: 6.2757036725951645\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 6.53312459865944\n",
      "The RMSE for train set: 6.07398231419475\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 5.07760733692771\n",
      "The RMSE for train set: 6.409997504587457\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 7.216509823456761\n",
      "The RMSE for train set: 5.888186036921103\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 6.378646232708767\n",
      "The RMSE for train set: 6.125358978750748\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 5.783630797121149\n",
      "The RMSE for train set: 6.276431210911164\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 6.534651426834456\n",
      "The RMSE for train set: 6.074695869230915\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 5.073628235109522\n",
      "The RMSE for train set: 6.4106810205603\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 7.220109971216243\n",
      "The RMSE for train set: 5.888910791739661\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 6.383096197593028\n",
      "The RMSE for train set: 6.126259529696248\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 5.778715914339649\n",
      "The RMSE for train set: 6.277362180852432\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 6.536355808573059\n",
      "The RMSE for train set: 6.075609071082187\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 5.069892949590023\n",
      "The RMSE for train set: 6.4115557940798436\n",
      "Time taken: 0.23 seconds\n",
      "The RMSE for test set: 7.22385915098465\n",
      "The RMSE for train set: 5.8898383452943746\n",
      "Time taken: 0.27 seconds\n",
      "The RMSE for test set: 6.387719081232842\n",
      "The RMSE for train set: 6.127355082611821\n",
      "Time taken: 0.28 seconds\n",
      "The RMSE for test set: 5.774073641332734\n",
      "The RMSE for train set: 6.278494598156424\n",
      "Time taken: 0.24 seconds\n",
      "The RMSE for test set: 6.538238395310396\n",
      "The RMSE for train set: 6.07672000918685\n",
      "Time taken: 0.26 seconds\n",
      "The RMSE for test set: 5.066400047462724\n",
      "The RMSE for train set: 6.412620024019573\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 7.227755621393176\n",
      "The RMSE for train set: 5.8909667677344615\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 6.3925097062334215\n",
      "The RMSE for train set: 6.128643764930556\n",
      "Time taken: 0.26 seconds\n",
      "The RMSE for test set: 5.76970242382905\n",
      "The RMSE for train set: 6.279826477251\n",
      "Time taken: 0.25 seconds\n",
      "The RMSE for test set: 6.540296804982223\n",
      "The RMSE for train set: 6.078026787613565\n",
      "Time taken: 0.26 seconds\n",
      "The RMSE for test set: 5.063147084343069\n",
      "The RMSE for train set: 6.413871895985082\n",
      "Time taken: 0.26 seconds\n",
      "The RMSE for test set: 7.2317978031351275\n",
      "The RMSE for train set: 5.892294142306126\n",
      "Time taken: 0.27 seconds\n",
      "The RMSE for test set: 6.397467468675234\n",
      "The RMSE for train set: 6.1301237042211225\n",
      "Time taken: 0.29 seconds\n",
      "The RMSE for test set: 5.76559967805478\n",
      "The RMSE for train set: 6.281355833997442\n",
      "Time taken: 0.29 seconds\n",
      "The RMSE for test set: 6.542529365390484\n",
      "The RMSE for train set: 6.079527504624304\n",
      "Time taken: 0.27 seconds\n",
      "The RMSE for test set: 5.060133637174462\n",
      "The RMSE for train set: 6.415309624097479\n",
      "Time taken: 0.37 seconds\n",
      "The RMSE for test set: 7.235984061236607\n",
      "The RMSE for train set: 5.893818542353937\n",
      "Time taken: 0.35 seconds\n",
      "The RMSE for test set: 6.402589907279604\n",
      "The RMSE for train set: 6.131793027081824\n",
      "Time taken: 0.36 seconds\n",
      "The RMSE for test set: 5.761764386221491\n",
      "The RMSE for train set: 6.283080682322445\n",
      "Time taken: 0.35 seconds\n",
      "The RMSE for test set: 6.544933821454904\n",
      "The RMSE for train set: 6.081220263615567\n",
      "Time taken: 0.35 seconds\n",
      "The RMSE for test set: 5.057356792841765\n",
      "The RMSE for train set: 6.416931388139427\n",
      "Time taken: 0.31 seconds\n",
      "The RMSE for test set: 7.240312746994153\n",
      "The RMSE for train set: 5.895538035193161\n",
      "Time taken: 0.30 seconds\n",
      "The RMSE for test set: 6.407875497198125\n",
      "The RMSE for train set: 6.133649861029209\n",
      "Time taken: 0.33 seconds\n",
      "The RMSE for test set: 5.758192661510679\n",
      "The RMSE for train set: 6.2849990420421395\n",
      "Time taken: 0.30 seconds\n",
      "The RMSE for test set: 6.5475093625662755\n",
      "The RMSE for train set: 6.0831031525721775\n",
      "Time taken: 0.30 seconds\n",
      "The RMSE for test set: 5.054815522553486\n",
      "The RMSE for train set: 6.418735396017001\n",
      "Time taken: 0.28 seconds\n",
      "The RMSE for test set: 7.2447823195761405\n",
      "The RMSE for train set: 5.8974507025877\n",
      "[6.153584829912752, 6.153687714418621, 6.153995044372916, 6.154504965681742, 6.155215574238558, 6.156124984201018, 6.157231296341826, 6.158532613617266, 6.160027041858856, 6.161712679270485, 6.163587630849646]\n",
      "[6.1991679452636195, 6.198609339702489, 6.198244494683149, 6.198085755754812, 6.198133332598028, 6.1983840042160825, 6.198837357346375, 6.199490764504578, 6.2003428421063145, 6.201391530958384, 6.202635072680941]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "RMSE_avg_train = []\n",
    "RMSE_avg_test = []\n",
    "\n",
    "for L in lambdas:\n",
    "    kf = KFold(n_splits=5)\n",
    "    RMSE_sub_train = []\n",
    "    RMSE_sub_test = []\n",
    "    for train_idx, test_idx in kf.split(X_train_data):\n",
    "\n",
    "        X_sub_train = X_train_data[train_idx]\n",
    "        X_sub_test = X_train_data[test_idx]\n",
    "        \n",
    "        y_sub_train = y_train_data[train_idx]\n",
    "        y_sub_test = y_train_data[test_idx]\n",
    "        \n",
    "        alpha, beta_arr, _, _ = ridge_regularization_multivariate_ols(X_sub_train, y_sub_train, 0.01, 1000, L)\n",
    "        \n",
    "        coefs = []\n",
    "        coefs.append(alpha[0])\n",
    "        for j in range(len(beta_arr)):\n",
    "            coefs.append(beta_arr[j][0])\n",
    "            \n",
    "            \n",
    "        X_mat_test = np.c_[np.ones((len(X_sub_test), 1)), X_sub_test]\n",
    "        y_hat_test = X_mat_test.dot(coefs)\n",
    "        print(\"The RMSE for test set:\", compute_rmse(y_hat_test, y_sub_test))\n",
    "        RMSE_sub_test.append(compute_rmse(y_hat_test, y_sub_test))\n",
    "        \n",
    "        X_mat_train = np.c_[np.ones((len(X_sub_train), 1)), X_sub_train]\n",
    "        y_hat_train = X_mat_train.dot(coefs)\n",
    "        print(\"The RMSE for train set:\", compute_rmse(y_hat_train, y_sub_train))\n",
    "        RMSE_sub_train.append(compute_rmse(y_hat_train, y_sub_train))\n",
    "    \n",
    "    \n",
    "    RMSE_avg_test.append(np.mean(RMSE_sub_test))\n",
    "    RMSE_avg_train.append(np.mean(RMSE_sub_train))\n",
    "\n",
    "print(RMSE_avg_train)\n",
    "print(RMSE_avg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa15cf460d0>,\n",
       " <matplotlib.lines.Line2D at 0x7fa15cf46190>]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lambdas, RMSE_avg_train, 'bo', lambdas, RMSE_avg_test, 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lambda value that has the smallest RMSE for test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"the lambda value that has the smallest RMSE for test\")\n",
    "lambdas[RMSE_avg_test.index(np.min(RMSE_avg_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value of lambda minimizes your cross-validated (test) RMSE? Fix that value of lambda, and train a new model using all of your training data with that value of lambda (i.e., use the entire 80% of the data that you set aside in 4.1). Calcuate the RMSE for this model on the 20% of \"true\" test data. How does your test RMSE compare to the RMSE from 4.3, 4.2, 2.3, and to the RMSE from nearest neighbors? What do you make of these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.34 seconds\n",
      "The RMSE for test set: 6.054203888550373\n",
      "The RMSE for train set: 6.164663824851001\n"
     ]
    }
   ],
   "source": [
    "alpha, beta_arr, _, _ = ridge_regularization_multivariate_ols(X_train_data, y_train_data, 0.01, 1000, 3)\n",
    "        \n",
    "coefs = []\n",
    "coefs.append(alpha[0])\n",
    "for j in range(len(beta_arr)):\n",
    "    coefs.append(beta_arr[j][0])\n",
    "            \n",
    "            \n",
    "X_mat_test = np.c_[np.ones((len(X_test_data), 1)), X_test_data]\n",
    "y_hat_test = X_mat_test.dot(coefs)\n",
    "print(\"The RMSE for test set:\", compute_rmse(y_hat_test, y_test_data))\n",
    "\n",
    "X_mat_train = np.c_[np.ones((len(X_train_data), 1)), X_train_data]\n",
    "y_hat_train = X_mat_train.dot(coefs)\n",
    "print(\"The RMSE for train set:\", compute_rmse(y_hat_train, y_train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJRo-C3kjl7U"
   },
   "source": [
    "The RMSE values for 4.2, 4.3, and 4.4 are pretty close, but we see a bigger difference with the result from 2.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izzKtl3_jl7U"
   },
   "source": [
    "###  Extra Credit 2: AdaGrad\n",
    "\n",
    "AdaGrad is a method to implement gradient descent with different learning rates for each feature. Adaptive algorithms like this one are being extensively used especially in neural network training. Implement AdaGrad on 2.3 but now use CRIM, RM and DIS as independent variables. Standardize these variables before inputting them to the gradient descent algorithm. Tune the algorithm until you estimate the regression coefficients within a tolerance of 1e-1. Use mini-batch gradient descent in this implementation. In summary for each parameter (in our case one intercept and three slopes) the update step of the gradient (in this example $\\beta_j$) at iteration $k$ of the GD algorithm becomes:\n",
    "\n",
    "$$\\beta_j=\\beta_j -\\frac{R}{\\sqrt{G^{(k)}_j}}\\frac{\\partial J(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$$ where\n",
    "$G^{(k)}_j=\\sum_{i=1}^{k} (\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j})^2$ and $R$ is your learning rate. The notation $\\frac{\\partial J^{(i)}(\\alpha,\\beta_1,\\ldots)}{\\partial \\beta_j}$ corresponds to the value of the gradient at iteration $(i)$. Essentially we are \"storing\" information about previous iteration gradients. Doing that we effectively decrease the learning rate slower when a feature $x_i$ is sparse (i.e. has many zero values which would lead to zero gradients). Although this method is not necessary for our regression problem, it is good to be familiar with these methods as they are widely used in neural network training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "n-fxLK0Rjl7V"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAa0A7Zzjl7V"
   },
   "source": [
    "*Discuss your results here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "_INFO251_PS4_2022_Feb22.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
